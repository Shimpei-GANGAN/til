## BERT による自然言語処理入門
- [Github](https://github.com/stockmarkteam/bert-book)
- Transformers：BERT で処理を行うためのライブラリ
- PyTorch Lightning：学習や性能評価を効率的に行うためのライブラリ

## sec 1

### NLP
- 基礎的な技術
    - 形態素解析
    - 言語モデル
        - 文章の自然さを確立で表現する
    - 固有表現抽出
    - 文章の類似度比較
- 応用的な技術
    - 文章分類
    - 文章生成
    - 文章校正

### ML
ML の一般的な流れ

- データからタスクを解くのに有用な特徴量を抽出する
- 抽出した特徴をモデルに入力し、その出力から問題を解く

**深層学習**においては、特徴量抽出とその後のモデルによる処理が１つのモデルで完結する「**end-to-end**」と呼ばれる形態をとっており、その中で特徴量は自動的に抽出されるようになっている！

### ML による NLP
ニューラル言語モデルの特徴の１つは、文章や単語を「密なベクトル（ほとんどがゼロのベクトルではない）」に変換できるということ。**文章や単語を密なベクトルとして表現したもの**は**分散表現**と呼ばれる。ニューラル言語モデルから得られる分散表現は何らかの形で単語や文章の意味を反映していると考えられる。そのため、ニューラル言語モデルから得られる分散表現はデータの有用な特徴量として用いられる。

ニューラル言語モデルは、特徴量抽出器としての役割を担っている。

### BERT
BERT は 2018 年に Google から発表されたニューラル言語モデル。文脈を考慮した分散表現を作成できる。

例えば、BERTから得られる単語の分散表現は、同じ単語でも文脈（周りの単語）が変わると、それに応じて異なる値を取る。Attention により、離れた位置でも適切に取り入れることができる。


## sec 2
NN を用いた NLP

### 前処理
**トークン化**とは、文を適切な単位に分割すること。これを実現するツールを**トークナイザ**という。

NN へのトークン化

- 事前に適当な方法で入力として扱いたいトークンの集合（語彙）を作成し、これに含まれる各トークンに対して順番に ID を割り当てておく
- 渡されたトークンを語彙に従い ID に変換する

ここで含まれないトークンは未知語と呼ばれ、情報が失われてしまう。

次の３つの分割方法がある

- 単語単位で分割する
- 文章単位で分割する
- 単語単位で分割した後にサブワード単位で分割する

#### 単語分割
日本語の単語分割を行うシステムとして、MeCab や Sudachi, Juman などの**形態素解析ツール**が利用される。形態素解析ツールは、文を分割するだけでなく、**統語的情報**を付与する。

#### サブワード分割
計算量と未知語への対応のバランスが取れたトークン化。

eg：「大阪大学 ⇨ 大阪＋大学」

Byte Pair Encoding(BPE)


### ニューラル言語モデル
NN により実現される言語モデル。

ニューラル言語モデルを学習する過程で得られるトークンの分散表現は、タスク非依存の汎用的な言語的特徴（トークンの意味や、瀕死などの統語的情報）を表すとされ、その価値はさまざまなタスクへ応用されることで証明されている。

#### What is 言語モデル
言語モデルは、「文章の自然さを確率によって表現している」と捉えることができる。

#### ニューラル言語モデルの構築

##### 入力層
トークンを意味のある入力とするためには、それぞれを密なベクトル表現に変換する必要がある。その返還を行う層は**埋め込み層（embedding layer）**と呼ばれる。一般的に、トークンに対応づけられてベクトルは**分散表現**、あるいは**埋め込み表現（embedding）**と呼ばれる。

扱いたい語彙の大きさをN、分散表現として割り当てたいベクトルの次元をDとすると、埋め込み層は N*D の行列 E によって特徴づけられる。

##### 出力層
出力層では、N次元ベクトルを出力する。

##### 学習データ
ニューラル言語モデルの学習データは、自然言語で記述された任意の文章データ（コーパス）とトークナイザを用いることで、自動的に作成できる。

#### 表現学習手法としてのニューラル言語モデル
「似たような文脈で出現するトークンが、似たような分散表現を持つ」ように学習される。これは、「語の意味は、その周辺に出現する語によって表される」という**分布意味仮説**に基づいているとも言える。

これの裏を返すと、次のような苦手分野が現れてしまう。

- 似たような文脈で出現してしまう、対義語関係（暑い/寒い）
- そもそも文章に出現しにくい、自明な常識（バナナは黄色い）

分散表現の類似度を表す定量的な尺度としては、**コサイン類似度**が用いられることが多い。


### Word2Vec
トークンに対して文脈に依存せず一意な表現を与えるモデルの代表が Word2Vec。

2013 年に提案された、単語に対して武脈日依存の分散表現を学習するモデル。

Word2Vec により得られる分散表現は、単語間の意味的類似度を表すだけでなく、以下のような性質を示すことが知られている（**加法構成性**）

$$
v(日本)-v(東京)\approx v(フランス)-v(パリ)\\
v(日本)+v(首都)\approx v(東京)
$$

また、計算効率にも優れている。

Word2Vec では、**CBOW, skip-gram**と呼ばれる２つのモデルが提案されている

#### CBOW
Continuous Bag-Of-Words

#### Word2Vec の問題点
「各単語に対して一意に分散表現を与える」という性質は、多義語を扱い際に問題になる。

- 彼は舞台の上手に立った
- 彼は料理上手だ

また、文中の単語の分散表現の重み付き平均を利用することが多いが、その場合には語順が考慮されていないので、次では同一の分散表現が与えられる

- ジョンはボブに本を貸した
- ボブはジョンに本を貸した

### ELMO
トークンに対して文脈に応じた表現を与えるモデルの代表。

単語に文脈に応じた分散表現を付与することを**文脈か単語埋め込み（Contextualized Word Embedding）**と呼ぶ。

LSTM が用いられている。

#### LSTM
「１つのRNN」と情報の流れを制御する「３つの**ゲート**」

#### ELMo のモデル
双方向LSTM（Bidirectional LSTM）


## sec 3
BERT

RNN をベースとしたモデルでは、離れた位置にある情報を考慮して処理を行うのが難しいという問題があった。また、前方から順々に処理をするため並列で計算することができなかった！

### BERT の構造
BERTも「文章をトークンに分割したものを入力として受けて、それぞれのトークンに対応するベクトルを出力する」という点では、RNNをベースとしたモデルと同様。

### 入力形式

#### トークン化
CLS(classification embedding) と SEP
```
今日の天気は雪だった
⇨
'[CLS]', '今日','の',...,'た', '。', '[SEP]'
```

文章のペアを入力する場合には、その境界に`[SEP]`をおく

#### ベクトル化
Attention を用いると、全てのトークンを位置に関係なく同等に扱うので、入力に文章中での位置を表す情報を加える必要がある。

### 学習

#### 事前学習
文章データは、ラベルなしでーた

- マスク付き言語モデル
- Next Sentence Prediction
    - ２つの文章が与えられるようなタスク

#### ファインチューニング
タスク内容に応じてBERTに新しい分類器などを接続するなどして、タスクに特化したモデルを作ること。言語タスクにおいて、BERTは特徴抽出器のような働きをする。

このように、BERTの出力を分類器などに接続するだけで、精度の高い物が作れる。

##### 訓練方法
モデルのパラメータの初期値として、BERTのパラメータは事前学習で得られたパラメータを用い、新たに加えられた分類器のパラメータにはランダムな値を与える。そして、ラベル付きデータを用いて、**BERTと分類器の両方のパラメータを学習する**。

BERTでのパラメータは固定して、分類器のみ学習を行う方法もあるが、ファインチューニングの方がモデルの性能が良くなる傾向があることがわかっている。


## sec 4
Huggingface Transformers

### 環境
- Pytorch は Colaboratory に最初からインストールされている
- 次のライブラリも使用する
    - Transformers
        - ニューラル言語モデルのライブラリ
    - Fugashi
        - 日本語の形態素解析ツールの MeCab を Python から使えるようにしたもの
    - ipadic
        - MeCab で形態素解析を行う際に用いるじしょ

### Transformers
Huggingface 社が提供しているオープンソースのライブラリ。BERTをはじめとする様々なニューラルネットワークを用いた言語モデルが実装されている。

BERTを用いた処理は、典型的に次の２ステップ

1. トークナイザを用いて、文章をトークン化し、BERTに入力できるような形にする
1. 上で処理したデータをBERTに入力し、出力を得る

BERTの日本語モデルのトークナイザは、以下の流れでトークン化を行う

1. MeCab を用いて単語に分割する
1. WordPiece を用いて単語をトークンに分割する

特に指定しなければ、MeCab の辞書としては ipadic が用いられる

BERT にトークンを入力するときには、トークンそのものではなく、トークンをユニークな数字に置き換えた ID が用いられる

トークンの先頭に'##'がついていることがある。この'##'の記号は、単語が WordPiece によってサブワードに分割されたときに、単語の一番最初以外のトークンに付与される。

実際にトークンをBERTに入力する際には、トークンそのものではなく、トークンのIDを用いる。
文章をトークン化し、それぞれのIDに変換する処理を、以降は「符号化」と呼ぶこととする。

符号化するには、`encode()`を使う

複数の文章をまとめて処理するには、次のような追加の処理が必要

まず、それぞれのトークン列の長さ（系列長）を同じに揃える。系列長が揃える長さよりも短ければ、トークン列の末尾に特殊トークン`[PAD]`を必要な数だけ足す。

### Tokenizer
return_tensors='pt' という引数を加えると、数値配列がテンソルとして出力され、BERTにそのまま入力することができる。

**２つの文章を入力に使う際に効いてくる！？！？！**

### BERT 推論
BERTで推論のみを行うときには、次のようにBERTの処理を、torch.no_grad() で囲むようにする！

こうすることで途中結果が保存されなくなり、メモリや計算時間を減らすことができる

``` python
with torch.no_grad():
    output = bert(**encoding)
    last_hidden_state = output.last_hidden_state
```

BERT は GPU で処理を行なっているので、その出力も GPU に配置されている。実際には、これを CPU にうつしたりするには、以下のようにする

``` python
last_hidden_state = last_hidden_state.cpu()
last_hidden_state = last_hidden_state.numpy()
last_hidden_state = last_hidden_state.tolist()
```


## sec 5
文章の穴埋め

### BERT を用いた文章の穴埋め
Transformers で提供されているクラス **BertForMaskedLM** を用いる。

穴埋めのためには、文章の一部を特殊トークン`[MASK]`に置き換えたものを用意する

### 複数の穴埋めタスク
```
今日は [MASK] [MASK] へ行く
```

この場合、２通りの MASK に対しては、$32000^2$の組み合わせの候補は膨大になっている。そのため、近似的な方法、**貪欲法**などで解くことを考える。

貪欲方では、最終的なスコアが高くなるとは**言えない**！そこで、より性能の良い近似手法として**ビームサーチと呼ばれる方法がある**


## sec 6

### Colab
ディレクトリを移動するには、先頭を % にする！

``` python
!mkdir chap6
%cd ./chap6
```

### 文章分類
いわゆる「**ネガポジ判定**」と呼ばれるタスクが有名。ネガポジ判定は、文章に内在する感情を判定する**感情分析**の一種。

livedoor ニュースコーパスを用いた分類ではニュース記事を９個のカテゴリーに分類する

### BERT による文章分類
BertForSequenceClassification:

BERT には、学習と推論の２つのモードがある。

[livedoor ニュースコーパス](https://www.rondhuit.com/download.html#ldcc)

Colaboratory から Google Drive にあるファイルへのアクセスはあまり早くない。。。

### BERT のファインチューニングと性能評価
データセットを前処理し、BERTに入力可能な形式に整えておく必要がある

#### データローダ
PyTorch ではデータセットを、「データローダ」という形式にする。
ファインチューニングではある指定された数（バッチサイズ）のデータトラベルをデータセットから抜き出し、**ミニバッチ**と呼ばれるデータ処理の単位を作る。そして、ミニバッチに対する処理結果をもとに、パラメータを更新するということを繰り返し行う。**「データローダ」とは、データセットからミニバッチを取り出すためのもの**

`DataLoader` に `shuffle=True`を渡すことで、データセットからランダムにデータを抜き出してミニバッチを作ることが可能。これにより、ファインチューニングの**エポックごとにデータの並び順が変わり、パラメータが局所最適解にトラップされるのを防ぐ！**

BERT では最大で 512 のトークンまでしか受け入れることができない。

検証データとテストデータでは損失の勾配を計算する必要がないため、バッチサイズを大きくすることができる

#### PyTorch Lightning によるファインチューニングとテスト
PyTorch の**高レベル**の API である PyTorch Lightning を使う。

pl.LightningModule 作成時のポイントは、次の３つのメソッドを作成すること

- train_step
- validation_step
- test_step

これらの関数は、学習データ、検証データ、テストデータそれぞれのデータローだからミニバッチを受け取ったときにどのような処理をするのか定義している。

PyTorch Linghtning のモデル関数の中には、モデルやデータを GPU に載せるコードや、推論時の torch.no_grad() のコードは書く必要がなく、これらは自動的に実行される！！！

PyTorch Lightning では、Trainer というクラスを用いて学習を行う。

TensorBoard は、TensorFlow や PyTorch で利用できる可視化ツール


#### TensorBoard
[TensorBoard](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks?hl=ja)

> TensorBoard は、Colab や Jupyter などのノートブックエクスペリエンス内で直接使用できます。そのため、結果の共有や既存のワークフローへの TensorBoard の統合を行い、ローカルに何もインストールせずに TensorBoard を使用することができます。

