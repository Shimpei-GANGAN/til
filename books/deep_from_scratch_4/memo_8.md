## sec 8 DQN
DQN は、Q学習と NN を使った手法

「経験再生」と「ターゲットネットワーク」が使われる

### OpenAI Gym
[site](https://gym.openai.com/)

``` sh
$ pip install gym
```

> 状態と観測は異なります。状態とは、環境に関しての「完全な記述（情報）」です。状態が分かれば、マルコフ決定過程により次の状態と報酬の確率分布は完全に決定します。一方、観測とは状態の「部分的な記述」です。これは、エージェントから問題（世界）の一部だけが見えるような問題を想像するとわかりやすいでしょう。
ポーカーでも麻雀でも、知名度のある不完全情報ゲーム

### DQN のコア技術
Q学習では、推定値を使って推定値を更新する（ブートストラッピング）。まだ正確ではない推定値で、推定値を更新することになるため、Q学習（TD法）は不安定になりやすい。さらに NN のような表現力の高い関数近似手法が加わると、結果はさらに不安定になる
DQNはQ学習とNNを使った手法で、安定化のために、**経験再生（Experience Replay）**と**ターゲットネットワーク（Target Network）**という技術を使っている。

#### 経験再生
エージェントが経験したデータ $E_t=(S_t,A_t,R_t,S_{t+1})$ を一度「バッファ」に保存する。そして、Q関数を更新する際には、そのバッファから経験データをランダムに取り出して使う。

こうすることで、$E_t$と$E_{t+1}$の間の相関などが弱まり、偏りの少ないデータが得られる！

> この方針は**方策オフ型**の他の強化学習アルゴリズムでも使用できる。

#### 実装
バッファに保存する最大値を設定し、それを超えたものは古い順に削除していく。この FIFO(First in, First out) を実現するために、`collections.deque`を使用する

#### ターゲットネットワーク（Target Network）
教師あり学習では、入力に対する正解ラベルは不変。

Q学習では、$Q(S_t,A_t)$の値が$R_t+\gamma\max_a{Q(S_{t+1},a)}$（TDターゲット）となるようにQ関数を更新する。ここでは、TDターゲットが、教師あり学習における正解ラベルに相当。しかし、TDターゲットの値は、Q関数が更新されると変動する。

この、教師あり学習とQ学習の違いを埋めるための工夫が、ターゲットネットワーク。

まず、Q関数を表すオリジナルのネットワーク（`qnet`）を用意。それとは別に、もう一つ同じ構造のネットワーク（`qnet_target`）を用意。`qnet`は通常のQ学習によって更新を行う。一方で、`qnet_target`は定期的に`qnet`の重みと同期するようにして、それ以外は重みパラメータは固定したままにする。あとは、TDターゲットの値を`qnet_target`を使って計算すれば、教師ラベルであるTDターゲットの変動が抑えられる。安定する！

### DQN と Atari

#### 前処理
これまでは、MDP(マルコフ決定過程)を前提としてきたが、今回の場合は MDP の要件は満たさない。なぜなら、画像だけからでは、ボールがどの方向に進んでいるかわからないから。もちろん、ボールの進行方向が不明のままでは最適な行動は行えない。このような問題は、**POMDP（部分観測マルコフ決定過程）**という。

> POMDP to MDP ??
- 4 フレームの連続する画像を合わせ、それを１つの状態とする
- RNN を使う

などが考えられる

#### CNN
前節のカートポールでは、全結合層からなる NN を使った。一方、Atari のような画像データを扱う場合には CNN が有効。CNN は、畳み込み演算を使った NN

なぜここではプーリング層を使わない？

#### その他の工夫
- GPU の使用
- $\varepsilon$の調整
  - DQN の論文では、最初の 100万ステップまでは$\varepsilon$を1.0から0.1まで線形に減少させ、それ以降は$\varepsilon$=0.1で固定
- 報酬クリップ（Reward Clipping）

### DQN の拡張
- Double DQN
- 優先度付き経験再生（Prioritized Experience Replay）
  - 経験データをバッファに保存するときに、TD誤差の絶対値を計算する
  - TD誤差が大きいほど学習すべきことが残っている状態なので、多くの確率で取り出す
- Dueling DQN









## メモ
- NN のような表現力の高い関数近似手法


