## sec 5 モンテカルロ法
DP を使うには、環境のモデル（状態遷移確率と報酬関数）が既知である必要がある。

モンテカルロ法（Monte Carlo method）とは、データのサンプリングを繰り返し行って、その結果から推定する手法の総称。

強化学習では、モンテカルロ法を使うことで、経験から価値関数を推定することができる。ここでの経験とは、「状態、行動、報酬」

### モンテカルロ法
確立分布として表されたモデルは**分布モデル（distribution model）**と呼ぶことができる。
モデルの表し方は、分布モデル以外にも、**サンプルモデル（sample model）**がある。これはサンプリングさえできれば良いというものであり、確率分布からサンプリングを行えることから、分布モデルの方がサンプルモデルよりも「大きな存在」であると言える。

### 方策評価
方策に従ってエージェントを実際に行動させた場合の、収益がサンプルデータになる。そのようなサンプルデータをたくさん集めて、その平均を求めるのがモンテカルロ法

$$
V_{\pi}(s)=\frac{G^{1}+G^{2}+\dots+G^{n}}{n}
$$

後ろから順に収益を求めることで、重複した計算を省くことができる！


### 方策制御
現在の価値関数を用いて、次の遷移先の中から価値関数の値が最大となる行動を選ぶ。これが「greedy」の意味。

$$
\pi(s)=\argmax_a\sum_{s^\prime}p(s^\prime|s,a)\{r(s,a,s^\prime+\gamma{V(s^\prime)})\}
$$

一般の問題においては、$p,r$は既知ではない。そこで、状態関数の代わりに$Q$関数（行動価値関数）を用いる。$Q$関数は「行動と状態」のペアからなる価値関数・

$$
q_{\pi}(s,a)=\mathbb{E_{\pi}}[G_t|S_t=s,A_t=a]
$$

$Q$関数は、状態$s$で行動$a$を行った後に、方策$\pi$にしたがって行動し続けた場合に得られる収益の期待値のこと。

この$q_\pi$を用いると、

$$
\pi(s)=\argmax_aq_\pi(s,a)
$$

のようになる。これより、環境のモデルがわからない場合は、「$Q$関数」に対して評価と改善を行うことになる。

モンテカルロ法 + $\varepsilon$-greedy法

### 重点サンプリングと方策オフ型
自分とは別の場所で得られた経験から、自分の方策を改善するアプローチを**方策オフ型（off-policy）**といい、自分で得た経験から自分の方策を改善する場合は**方策オン型(on-policy)**という。

ターゲット方策と挙動方策が異なる場合、つまり方策オフ型を以下では扱う。挙動方策から得られたサンプルデータを使って、ターゲット方策に関連する期待値を求めるために、**重点サンプリング（Importance Sampling）**というテクニックを使う。

#### 重点サンプリング
ある確率分布の期待値を、別の確率分布からサンプリングしたデータを使って計算する手法。

$$
\mathbb{E_\pi}[x]=\sum{x\pi(x)}
$$

を例として考える。

この期待値をモンテカルロ法を使って近似するには、$x$を確率分布$\pi$からサンプリングして、その平均を取る

$$
\rm{sampling:}x^{(i)}\sim\pi\quad(i=1,2,\dots n)
$$
$$
\mathbb{E_\pi}[x]=\frac{x^{(1)}+x^{(2)}+\dots x^{(n)}}{n}
$$

では、$x$が別の確率分布からサンプリングされた場合を考えよう。

$$
\mathbb{E_\pi}[x]=\sum{x\pi(x)}\\
=\sum{x\frac{b(x)}{b(x)}\pi(x)}\\
=\sum{x\frac{\pi(x)}{b(x)}b(x)}\\
=\mathbb{E_b}[x\frac{\pi(x)}{b(x)}]
$$

各$x$に、重み$\rho=\frac{\pi(x)}{b(x)}$がかけられている



