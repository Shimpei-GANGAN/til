## sec 5 モンテカルロ法
DP を使うには、環境のモデル（状態遷移確率と報酬関数）が既知である必要がある。

モンテカルロ法（Monte Carlo method）とは、データのサンプリングを繰り返し行って、その結果から推定する手法の総称。

強化学習では、モンテカルロ法を使うことで、経験から価値関数を推定することができる。ここでの経験とは、「状態、行動、報酬」

### モンテカルロ法
確立分布として表されたモデルは**分布モデル（distribution model）**と呼ぶことができる。
モデルの表し方は、分布モデル以外にも、**サンプルモデル（sample model）**がある。これはサンプリングさえできれば良いというものであり、確率分布からサンプリングを行えることから、分布モデルの方がサンプルモデルよりも「大きな存在」であると言える。

### 方策評価
方策に従ってエージェントを実際に行動させた場合の、収益がサンプルデータになる。そのようなサンプルデータをたくさん集めて、その平均を求めるのがモンテカルロ法

$$
V_{\pi}(s)=\frac{G^{1}+G^{2}+\dots+G^{n}}{n}
$$

後ろから順に収益を求めることで、重複した計算を省くことができる！


### 方策制御
現在の価値関数を用いて、次の遷移先の中から価値関数の値が最大となる行動を選ぶ。これが「greedy」の意味。

$$
\pi(s)=\argmax_a\sum_{s^\prime}p(s^\prime|s,a)\{r(s,a,s^\prime+\gamma{V(s^\prime)})\}
$$

一般の問題においては、$p,r$は既知ではない。そこで、状態関数の代わりに$Q$関数（行動価値関数）を用いる。$Q$関数は「行動と状態」のペアからなる価値関数・

$$
q_{\pi}(s,a)=\mathbb{E_{\pi}}[G_t|S_t=s,A_t=a]
$$

$Q$関数は、状態$s$で行動$a$を行った後に、方策$\pi$にしたがって行動し続けた場合に得られる収益の期待値のこと。

この$q_\pi$を用いると、

$$
\pi(s)=\argmax_aq_\pi(s,a)
$$

のようになる。これより、環境のモデルがわからない場合は、「$Q$関数」に対して評価と改善を行うことになる。

モンテカルロ法 + $\varepsilon$-greedy法

### 重点サンプリングと方策オフ型
自分とは別の場所で得られた経験から、自分の方策を改善するアプローチを**方策オフ型（off-policy）**といい、自分で得た経験から自分の方策を改善する場合は**方策オン型(on-policy)**という。

ターゲット方策と挙動方策が異なる場合、つまり方策オフ型を以下では扱う。挙動方策から得られたサンプルデータを使って、ターゲット方策に関連する期待値を求めるために、**重点サンプリング（Importance Sampling）**というテクニックを使う。

#### 重点サンプリング
ある確率分布の期待値を、別の確率分布からサンプリングしたデータを使って計算する手法。

$$
\mathbb{E_\pi}[x]=\sum{x\pi(x)}
$$

を例として考える。

この期待値をモンテカルロ法を使って近似するには、$x$を確率分布$\pi$からサンプリングして、その平均を取る

$$
\rm{sampling:}x^{(i)}\sim\pi\quad(i=1,2,\dots n)
$$
$$
\mathbb{E_\pi}[x]=\frac{x^{(1)}+x^{(2)}+\dots x^{(n)}}{n}
$$

では、$x$が別の確率分布からサンプリングされた場合を考えよう。

$$
\mathbb{E_\pi}[x]=\sum{x\pi(x)}\\
=\sum{x\frac{b(x)}{b(x)}\pi(x)}\\
=\sum{x\frac{\pi(x)}{b(x)}b(x)}\\
=\mathbb{E_b}[x\frac{\pi(x)}{b(x)}]
$$

各$x$に、重み$\rho=\frac{\pi(x)}{b(x)}$がかけられている


## sec 6 TD 法
MC 法は、環境のモデルを使わずに方策を評価できるものの、エピソードの「おわり」に辿り着いてから出ないと、価値関数の更新ができない（終わりで初めて収益が確定するから）
そのため、連続タスクの場合、MC 法は使うことができない！

Temporal Difference, エピソードの終わりを待つのではなく、一定の時間が進むごとに学習を行う

### TD 法での方策評価
MC 法 + DP のような手法

TD 法のアイデア

- MC 法のように、サンプリングされたデータを使うこと
- DP のように、次の情報（次の行動と状態の情報）だけを使うこと

TD 法の更新方法

$$
V^{\prime}_\pi(S_t)\leftarrow V_\pi(S_t)+\\
\alpha(R_t+\gamma V_\pi(S_{t+1})-V_\pi(S_t))
$$

$R_t+\gamma V_\pi(S_{t+1})$は**TD ターゲット**と呼ばれる

MC と異なり、TD は、推定値（$V_\pi$）で推定値を更新している：ブートストラッピング（bootstrapping）

### SARSA (サルサ)
Q 関数は、状態と行動のペアデータを１つの単位とする。

方策 ON と OFF のどちらも実装できる

### Q 学習
- TD 法
- 方策オフ型
- 重点サンプリングを行わない

ベルマン方程式 → SARSA
ベルマン最適方程式 → Q学習

#### SARSA とベルマン方程式の関係
SARSA は、ベルマン方程式のサンプリング版と見なすことができる。全ての遷移ではなく、ある１つのサンプリングされたデータを使うということ

#### ベルマン最適方程式とQ学習
Q 関数のベルマン最適方程式

$$
q_*(s,a)=\\
\sum_{s'}p(s'|s,a)\{r(s,a,s')+\gamma\max_{a'}q_*(s'|a')\}
$$

ベルマン最適方程式は、ベルマン方程式とは異なり、max 演算子が使われている

$$
Q^{\prime}_\pi(S_t,A_t)\leftarrow 
Q_\pi(S_t,A_t)+\alpha(R_t+\gamma\max_a Q(S_{t+1},a)-Q(S_t,A_t))
$$

上記に基づき Q 関数を繰り返し更新することで、最適方策における Q 関数へと近づいていく

重要な点は、行動 $A_{t+1}$ が Q 関数の最大値によって選ばれていること。何からの方策によってサンプリングされるのではなく、行動 $A_{t+1}$ は max 演算子によって選ばれる。そのため、方策オフ型の手法でありながら、重点サンプリングによる補正を行う必要はない！（やった！）

まとめ

> Q学習は方策オフ型の手法。ターゲット方策と挙動方策の２つを持ち、挙動方策 b には「探索」を行わせる。よく用いられる挙動方策は、現在の推定値である Q 関数を $\varepsilon$-greedy 化した方策。挙動方策が決まれば、それに従い行動を行いサンプルデータを集め、その度にQ関数を更新する！

#### エージェントの方策について
エージェントの行動の決め方も「分布モデル」と「サンプルモデル」があり、サンプルモデルの方がシンプルな実装になる

サンプルモデルの場合、方策を確率分布として保持していない（方策自体保持していない）



