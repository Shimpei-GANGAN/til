## sec 9 方策勾配法

これまで、Q学習やSARSA、モンテカルロ法について学んできたが、これらは**価値ベースの手法（Value-based method）**に分類される。

> 価値ベースの手法では、 一般化方策反復 というアイデアにもとづき、最適方策を見つけることが多く行われます。具体的には、価値関数を評価し、価値関数を使って方策を改善します。そして、その改善した方策で価値関数を求めるといったプロセスを繰り返すことで、徐々に最適方策に近づきます。

価値関数を経由せずに方策を直接得る手法もあり、**方策ベースの手法（Policy-based method）**と呼ばれる。中でも、方策をNNなどでモデル化して、勾配を使って方策を最適化する手法は**方策勾配法(Policy Gradient Method)**と呼ばれる

### 最も単純なケース

#### 導出
方策$\pi(a|s)$をNNで表すこととする。NNの全ての重みパラメータを$\theta$という記号で集約して表すこととする。そして、NNによる方策を$\pi_\theta(a|s)$で表すこととする。

方策をNNとして$\pi_\theta(a|s)$で表せば、次は方策$\pi_\theta(a|s)$を使って損失関数を設定する。損失関数を設定すれば、あとは損失関数を静消化するパラメータ$\theta$を見つける（最適化）。

エピソードタスク、方策$\pi_\theta$に従う場合を考え、次のような「状態、行動、報酬」からなる時系列データが得られたとする

$$
\tau = (S_0, A_0, R_0, S_1, A_1, R_1, \dots, R_{T+1})
$$


### REINFORCE
最も簡単な方策勾配法を改善した手法

### ベースライン（Baseline）
REINFORCEを改善するための技術

### Actor-Critic
価値ベースかつ方策ベースの手法

### 方策ベースの手法の利点

#### 方策を直接モデル化するので効率的
最終的に行いたいのは最適方策である。価値ベースの手法は、価値関数の推定を挟むため、少し非効率な感じはするよね？

#### 連続的な行動空間でも使える
これまで見てきた問題は全てが離散的な行動空間だった（カートポールの場合、左と右の２つ）。そのような離散行動空間では、行動はいくつかの候補の中から１つを選ぶこととなる。一方で、離散的な行動空間もありえる。

価値ベースの手法は、行動が離散空間になると適応が難しくなる。対策方策は、いくつか考えられ、ひとつの方法に、連続空間を離散化することがある。どの程度離散化するか、これをクオンタイズ（Quantize）という。

> 一方、方策ベースの手法であれば連続な行動空間にもシンプルに対応できます。たとえば、ニューラルネットの出力がガウス分布を想定することができます。その場合、方策をモデル化したニューラルネットは、ガウシアン分布の平均と分散を出力することが考えられます。そのニューラルネットが出力した平均と分散をもとにサンプリングすることで連続値が得られます（図9.13）。

1. 入力の状態
2. NN
3. 出力として平均（+分散ある時も）
4. この情報をもとにガウス分布で近似してサンプリング

#### 行動の選択確立がスムーズに変化する
価値ベースの手法では、エージェントの行動は$\varepsilon$-greedy法で選ばれることが多くある。その場合、Q関数の一番大きい行動が多く選ばれる。
この時、Q関数の更新によって、Q関数の最大値となる行動が変わると、行動のとり方が劇的に変わることになる。一方、方策ベースの手法は、ソフトマックス関数によって各行動の確率が決まる。そのため、方策のパラメータを更新していく過程でも各行動の確率はスムーズに変わる。このおかげで、方策勾配法の学習は安定する！

> [注]常に方策ベースの手法が価値ベースの手法を上回るということはなく、タスクによって得意・不得意がある


## 疑問
方策ベースと価値ベースで優劣はあるのか

