## sec 3
ベルマン方程式

エージェントの行動が確率的な振る舞いをする場合の価値関数を求めることが目標

### ベルマン方程式
$$G_t=R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+...\\
=R_t+\gamma (R_{t+1}+\gamma R_{t+2}+...)\\
=R_t+\gamma G_{t+1}$$

収益である、$G_t$と$G_{t+1}$の関係が導かれた！

収益の期待値である状態価値関数は、

$$
v_\pi=\mathbb{E}[G_t|S_t=s]\\
=\mathbb{E}[R_t+\gamma G_{t+1}|S_t=s]\\
=\mathbb{E}[R_t|S_t=s]+\mathbb{E}[G_{t+1}|S_t=s]\\
$$

丁寧に計算を追っていくと、次の**ベルマン方程式**を得る

$$
v_\pi=\sum_{a,s'}\pi(a|s)p(s'|s,a)\{r(s,a,s')+\gamma v_\pi(s')\}
$$

これは、「状態$s$の価値関数」と「その次にとり得る状態$s'$の価値関数」との関係性を表した式である。このベルマン方程式は、全ての状態$s$と全ての方策$\pi$について成り立つ。

### ベルマン方程式の意義
ベルマン方程式によって、無限に続く計算ー手に負えない計算ーを、有限の連立方程式に変換する例をみた。今回のように、ランダムな振る舞いがあったとしても、ベルマン方程式を使えば、その価値関数を求めることができる。

### Q関数とベルマン方程式
$$q_\pi(s,a)=\mathbb{E_\pi}[G_t|S_t=s,A_t=a]$$
Q関数は、状態$s$で行動$a$を行い、そのあとは方策$\pi$に従って行動する。その場合に得られる収益の期待値がQ関数である。

> $q_\pi(s,a)$の行動$a$は、方策$\pi$とは関係ない。$q_\pi(s,a)$の行動$a$は自由に決めることができ、その行動の後は、方策$\pi$にしたがって行動する。

**Q関数のベルマン方程式**

$$
q_\pi(s,a)=\\
\sum_{s'}p(s'|s,a)\{r(s,a,s')+\gamma\sum_{a'}\pi(a'|s')q_\pi(s'|a')\}
$$

### ベルマン最適方程式
ベルマン方程式は、ある方策$\pi$に対してなりたいつ方程式である。ただし、今求めたいのは最適方策であり、最適方策に関しては、ベルマン最適方程式が成立する！

$$
v_\pi(s)=\sum_a\pi(a|s)\sum_{s'}p(s'|s,a)\{r(s,a,s')+\gamma v_\pi(s')\}
$$

最適方策$\pi_*$については、決定論的な最適方策が必ず存在する。決定論的な方策とは、ある状態では常にある特定の行動を選ぶ方策のこと。つまり、ある状態においてある行動を選ぶ確率が１であり、それ以外の行動を選ぶ確率は０ということ。
そのためベルマン方程式では $a$ に関する和をとっているが、その中で計算に関与してくるのは１つの行動だけである。
よって、最適方策に対する**ベルマン最適方程式**は、次のように表される

$$
v_*(s)=\max_a\sum_{s'}p(s'|s,a)\{r(s,a,s')+\gamma v_\pi(s')\}
$$

#### Q関数におけるベルマン最適方程式
行動価値関数（Q関数）においても、同様にベルマン最適方程式を求めることができる

$$
q_*(s,a)=\\
\sum_{s'}p(s'|s,a)\{r(s,a,s')+\gamma\max_{a'}q_*(s'|a')\}
$$





