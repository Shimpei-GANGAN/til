## sec 1

### 機械学習の分類と強化学習
- Supervised Learning
- Unsupervised Learning
- Self-Supervised Learning
- Reinforcement Learning

### バンディット問題
bandit = スロットマシンの別称

$$q(A)=\mathbb{E}[R|A]$$

A という行動を取った時の、R (Reward)の期待値で、行動価値と呼ぶ

q(A) は「真の行動価値」を表し、Q(A) は「推論値」を表す、という使い分けを行う

#### 大数の法則
サンプル平均は、真の値に近づく。無限回サンプリングした場合、そのサンプル平均は真の値に一致する。

#### プレイヤーの戦略
以下のトレードオフの関係にある
- これまで実際にプレイした結果を利用し、最善と思われるスロットマシンをプレイする（=greedy な行動）
- スロットマシンの価値を精度よく推論するために、さまざまなスロットマシンを試すこと
  - 「greedy でない行動」を試すこと（探索）で、より良い選択を見つけ出したい


上記トレードオフの関係、つまり、探索と活用のバランスをいかにとるか、そこが強化学習のアルゴリズムのきもとなる

- イプシロングリーディー法

### 非定常問題
これまでの Bandit 問題は、定常問題（Stationary Problem）という問題設定に分類される。つまり、報酬の確率分布が定常である問題のこと。

定常問題では、 

$$Q_n=\frac{R_1+R_2+...+R_n}{n}$$

という式で表されていた。

上の例では、全ての報酬に対して 1/n の重みをかけていることになる。つまり、**全ての時間軸の報酬を均等に扱ってしまっている！**

ところが実際には、新しい報酬の重みほど大きくするべき！

行動価値のサンプル平均は、現状以下のようにかけていた

$$Q_n=Q_{n-1}+\frac{1}{n}(R_n-Q_{n-1})$$

この 1/n を $\alpha$ という固定値（たとえば 0.1 など）に変更する。

$$Q_n=Q_{n-1}+\alpha(R_n-Q_{n-1})$$

固定値による更新を行なった場合、n が小さくなるにつれて、指数関数的に重みが減少する。

$$Q_n=\sum^n_{k=1}\alpha(1-\alpha)^{n-k}R_k+(1-\alpha)^nQ_0$$

また、**上式の各報酬に対する重みは全て足すと１になる**！！！！！

つまり、

$$\sum^n_{k=1}\alpha(1-\alpha)^{n-k}+(1-\alpha)^n=1$$

である。この重みの和が１となることから、この計算は「重みつき平均」とも呼ばれる

> $Q_n$を求めるために、$Q_0$の値も使われていることには注意する必要がある！$Q_0$は行動価値の初期値であり、これは私たちが設定する値。$Q_n$はこの値の影響を受けてしまう。一方で、単純なサンプル平均の場合は、そのようなバイアスは発生しない。サンプル平均の場合は、最初の報酬を得るとユーザが与えた初期値は消えることになるから

- なぜ、サンプル平均の場合はユーザーの初期値は消えることになるのか

$$Q_1=Q_0+\frac{1}{1}(R_1-Q_1)=R_1$$


### まとめ
- 活用と探索のバランスを取ることが重要
- そのためのアルゴリズムとして$\varepsilon$-greedy法
- サンプル平均と重みつき平均
- 非定常問題の場合は重みつき平均の方が適している


## sec 2
バンディット問題では、エージェントがどのような行動を取っても、次に取り組む問題は変わらなかった。しかし囲碁などを考えてみてもわかるように、現実は盤面などの状況が刻一刻と変わっていく。

エージェントの行動によって状況が変わる問題を扱う。そのような問題は**マルコフ決定過程（Markov Decision Process, MDP）**として定式化される。


> 非定常状態はエージェントの行動に関係なく、時間が経過するに従って報酬の確率分布が変わった。一方で、これから取り組む問題は、**エージェントの行動によって**状況が変化する問題であり、別次元の問題である。

### MDP
- state: エージェントの置かれる状況

MDPには、「時間」という概念が必要になる。ある時刻にエージェントが行動を行い、その結果として、新しい状態へ遷移する。この時の時間の単位は「タイムステップ」などと呼ばれる。

目先の報酬ではなく、将来的に得られる報酬を最大化する。**報酬の総和の最大化**

$$S_0,A_0,R_0, S_1,A_1,R_1, S_2,A_2,R_2,...$$

### 定式化
以下の３つの要素を数式で表す

- 状態遷移
- 報酬関数
- 方策

#### 状態遷移
決定論的な遷移と確率的な遷移がある。
確率的な遷移において、$s$から行動$a$をとって$s'$に移動する確率は、次のように表せる

$$p(s'|s,a)$$

| の右側には「条件」を表す確率変数である s,a が書かれている。

「マルコフ性」：現在よりも過去の情報は必要ない。現在の状況とその時の　Action のみで未来の状況が決まる。

#### 報酬関数
報酬は「決定論的」に与えられている想定で進める。

$$r(s,a,s')$$

のように記述する。

#### 方策（policy）
エージェントはどのように行動を決定するか。豊作に関して重要な点は、エージェントは「現在の状態」だけに基づいて行動を決定していること。

> MDPのマルコフ性という性質は、エージェントへの制約ではなく、むしろ環境への制約として見ることができる。つはりは、環境側がマルコフ性を満たすように「状態」を保つ必要がある。

ここでも決定論的な方策と確率的な方策がある。
決定論的な方策は、関数として
$$a=\pi(s)$$
のように定義できる。
また、確率的に決まる方策に対しては、次のような式で表される
$$a=\pi(a|s)$$
これは、ある状態$s$にいるときに行動$a$を取る確率を表す。


### MDP を利用した強化学習の目標
最適方策（Optimal Policy）を見つけること。最適方策とは、長期的に得られる報酬の総和の最大化

#### エピソードタスクと連続タスク
エピソードタスクは「終わり」のある問題（囲碁など）。

#### 収益（return）
収益$G_t$は以下のように報酬$R_t$を用いて定義される。
$$G_t=R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+...$$
$\gamma$は**割引率**と呼ばれ、0-1の間の実数を設定する。

割引率を導入する理由は、連続タスクと想定した時に、収益が無限大になることを防ぐため。
（また、近い将来の収益は確実性が高く遠い将来の収益は不確実性が高いから？）

> 人は目の前の誘惑に抗うことができないことが度々ありますが、その原理は割引率によって解釈できます。割引率によって将来の報酬が指数関数的に小さくなるとすれば、即時的な誘惑の方が魅力的に感じてしまいます。

#### 状態価値関数と行動価値関数
エージェントと環境が「確率的」であることを反映して、収益も「確率的」な振る舞いをする。

このような確率的な挙動に対しては期待値を定義するのが良い

$$v_\pi(s)=\mathbb{E}[G_t|S_t=s,\pi]$$

状態$S_t$が$s$であり、エージェントの方策が$\pi$であることを条件としている。ここでは、収益の期待値を$v_\pi(s)$という特別な記号で呼ぶこととする。$v_\pi(s)$は**状態価値関数**と呼ばれる。

なお、以下のように書く場合もある

$$v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]$$

状態価値関数は、方策と状態を条件としていた。これに追加して行動aも条件に追加することも考えられる。これが**行動価値関数**である。

$$q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]$$

通称**Q関数**

#### 最適方策と最適行動関数
へー

MDP において、最適な決定論的方策（全ての状態において、他のどの方策よりも状態価値観数の値が大きい）が少なくとも１つ存在する！！


### MDP の例








## 疑問


