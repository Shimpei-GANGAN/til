## sec 1

### 機械学習の分類と強化学習
- Supervised Learning
- Unsupervised Learning
- Self-Supervised Learning
- Reinforcement Learning

### バンディット問題
bandit = スロットマシンの別称

$$q(A)=\mathbb{E}[R|A]$$

A という行動を取った時の、R (Reward)の期待値で、行動価値と呼ぶ

q(A) は「真の行動価値」を表し、Q(A) は「推論値」を表す、という使い分けを行う

#### 大数の法則
サンプル平均は、真の値に近づく。無限回サンプリングした場合、そのサンプル平均は真の値に一致する。

#### プレイヤーの戦略
以下のトレードオフの関係にある
- これまで実際にプレイした結果を利用し、最善と思われるスロットマシンをプレイする（=greedy な行動）
- スロットマシンの価値を精度よく推論するために、さまざまなスロットマシンを試すこと
  - 「greedy でない行動」を試すこと（探索）で、より良い選択を見つけ出したい


上記トレードオフの関係、つまり、探索と活用のバランスをいかにとるか、そこが強化学習のアルゴリズムのきもとなる

- イプシロングリーディー法

### 非定常問題
これまでの Bandit 問題は、定常問題（Stationary Problem）という問題設定に分類される。つまり、報酬の確率分布が定常である問題のこと。

定常問題では、 

$$Q_n=\frac{R_1+R_2+...+R_n}{n}$$

という式で表されていた。

上の例では、全ての報酬に対して 1/n の重みをかけていることになる。つまり、**全ての時間軸の報酬を均等に扱ってしまっている！**

ところが実際には、新しい報酬の重みほど大きくするべき！

行動価値のサンプル平均は、現状以下のようにかけていた

$$Q_n=Q_{n-1}+\frac{1}{n}(R_n-Q_{n-1})$$

この 1/n を $\alpha$ という固定値（たとえば 0.1 など）に変更する。

$$Q_n=Q_{n-1}+\alpha(R_n-Q_{n-1})$$

固定値による更新を行なった場合、n が小さくなるにつれて、指数関数的に重みが減少する。

$$Q_n=\sum^n_{k=1}\alpha(1-\alpha)^{n-k}R_k+(1-\alpha)^nQ_0$$

また、**上式の各報酬に対する重みは全て足すと１になる**！！！！！

つまり、

$$\sum^n_{k=1}\alpha(1-\alpha)^{n-k}+(1-\alpha)^n=1$$

である。この重みの和が１となることから、この計算は「重みつき平均」とも呼ばれる

> $Q_n$を求めるために、$Q_0$の値も使われていることには注意する必要がある！$Q_0$は行動価値の初期値であり、これは私たちが設定する値。$Q_n$はこの値の影響を受けてしまう。一方で、単純なサンプル平均の場合は、そのようなバイアスは発生しない。サンプル平均の場合は、最初の報酬を得るとユーザが与えた初期値は消えることになるから

- なぜ、サンプル平均の場合はユーザーの初期値は消えることになるのか

$$Q_1=Q_0+\frac{1}{1}(R_1-Q_1)=R_1$$


### まとめ
- 活用と探索のバランスを取ることが重要
- そのためのアルゴリズムとして$\varepsilon$-greedy法
- サンプル平均と重みつき平均
- 非定常問題の場合は重みつき平均の方が適している


## 疑問


