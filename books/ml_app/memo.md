# ML による実用アップ
- [Code](https://github.com/hundredblocks/ml-powered-applications)
- [fast.ai](https://www.fast.ai/)
- [data science from scratch](http://math.ecnu.edu.cn/~lfzhou/seminar/[Joel_Grus]_Data_Science_from_Scratch_First_Princ.pdf)

## Intro

### ML の全プロセス
1. 適切なMLアプローチの特定
    * 適切な成功基準を設定し、適切な初期データセットとモデルを選定すること
2. 初期プロトタイプの作成
    * モデルに取り組む前に、E2Eのプロトタイプを構築する
    * プロトタイプが構築されると、MLの必要性の有無が判明し、モデルを学習させるためのデータセットの収集を開始できる
3. モデルの反復
    * データセットが用意できたら、モデルの学習を行い、その欠点を評価する
    * エラー分析と実装を交互に繰り返すこと
4. デプロイと監視
    * デプロイされたモデルは、思わぬエラーに遭遇することがよくある
    * モデルエラーの軽減と監視を行う

## sec 1
MLはさまざまなアプリケーションの世界を切り開くが、MLで解決できる問題と解決すべき問題について考えることが重要。

### 何が可能であるかを考える


## sec 2
計画の作成

### 成功度合いの測定

#### 製品メトリクス
- 最終的に大事
- 具体例
    - ユーザ数
    - 提供した推薦内容のクリックスルー率（CTR: Click Through Rate）

#### モデルメトリクス
製品が構築中で、まだデプロイされてない場合、製品メトリクスは測定できない。それでも進捗を測定するために、**オフラインメトリクス**または**モデルメトリクス**と呼ばれる指標を定義することが重要。オフラインメトリクスとは、モデルをユーザに公開することなく評価でき、製品メトリクスや目標と可能な限り相関するもの

- モデルと製品の相互作用
    - モデルの結果が信頼度の閾値以下の場合に表示を省略する
        - ユーザが入力した分を自動補完するモデル等
    - 最上位の予測に加えて、他の幾つかの予測またはヒューリスティックを提示
    - モデルがまだ実験段階であることをユーザに伝え、フィードバックを提供する機会を与える

モデルの有用性を評価するときには、その最適化メトリクスを超えたところに目を向ける必要がある

MLエディタの文章作成支援を考える。たいていのモデルには、得意な入力とそうでないものがある。製品の観点から、支援ができないのであれば、害を及ぼさないようにする必要がある。

モデルを最新の**状態に保つ**ことがどれだけ難しいかを考えておく必要がある！

そうなると、再学習のしやすさも重要だよなぁ

#### スピード
理想的には、予測を素早く行う必要がある

ここのケースでは、LSTMはランダムフォレストより約3倍遅くなるらしい


### スコープと課題の推定
MLのパフォーマンスの多くはモデルメトリクスで報告されるが、実際には製品メトリクスを使用するべき。通常MLで成功するには、問題の内容をよく理解し、良いデータセットを取得し、適切なモデルを構築する必要がある。

- データの調査
    - EDA: Exploratory Data Analysis
        - データ可視化の手法
        - データの傾向理解


#### 巨人の肩に立つ
**MLプロジェクトを始める最良の方法は、既存の結果を理解して再現すること**。公開された類似のモデル、類似のデータセット、またはその両方を使った実装を探す。

多くの場合、プロジェクトに多大なリソースを投入する前に、説得力のある概念検証（PoC: Proof oc Concept）の実行を行う。たとえば、時間と予算を使ってデータのラベルづけを行う前に、そのデータから学習するモデルが実際に構築できることを示す必要がある。

ML は反復プロセスであり、モデルがどのように失敗したかを見ることが最も早く進歩する方法である。

### シンプルに始める



### memo
- 常にMLが必要なわけではない
    - MLの恩恵を受ける可能性のある機能でさえ、最初のバージョンではヒューリスティックを使うだけで良い
    - 多くの場合、ヒューリスティックは特徴量を構築する最も速い方法
