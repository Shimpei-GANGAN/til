## sec 2
誤差伝播方は連鎖率を計算グラフに適応したものである。

Tensorflow の強力な自動美文機能を活用できるAPIはGradientTape。GradientTapeはPythonのスコープで、その中で実行されるテンソル演算を計算グラフとして記録する。

## sec 3

### TensorFlow, Keras
TensorFlowはPythonベースのオープンソースのMLプラットフォームであり、主にGoogleによって開発されている。

KerasはTensorFlowをベースに構築されたPython用のディープラーニングAPI。Kerasはユーザーエクスペリエンスを一番に考えているらしい。Kerasはマシンではなく人間のためのAPIであり、認知的負荷を減らすためにベストプラクティスに従っている。つまり、一貫性のあるワークフローを提供し、一般的なユースケースで必要となるアクションの数を最小限に抑え、ユーザーエラーに対して明確ですぐに行動に移せるフィードバックを提供する。

### 環境
ローカルとクラウドのどちらで実行する場合でも、Unixワークステーションを使うほうがよいでしょう


## sec 4

### 二値分類の例
交差エントロピーとは、２つの確率分布の間の距離を表す情報理論の尺度。

### 回帰（Regression）
テストデータの正規化に使われる値は、訓練データを使って計算する。
たとえデータの正規化のような単純なものであっても、ワークフローでは決してテストデータを使って計算した値を使うべきではない。

小さいモデルを使うことは過学習を抑制する方法の１つ

### k分割交差検証
利用できるデータが少ない場合、モデルを正確に評価するのに k分割交差検証を使う。
k-fold cross-validation でパラメータを決定した後は、全データを使って訓練し直す。


## sec 5
機械学習の根本的な課題は、最適化と汎化の間の緊張関係にある。

最適化とは、訓練データの性能ができるだけ良くなるようモデルを調整するプロセスのことで、機械学習の学習にあたる。汎化（generalizatioin）とは、訓練済みのモデルを未知のデータで実行したときの性能がどれくらい良いかを表す。

汎化性能を向上させることが機械学習の目標であるが、汎化は制御できない。

### 過学習を起こしやすいもの
- ノイズを含んでいる訓練データ
  - 自然のデータにはノイズがのる
- 曖昧な特徴量
  - 人間がラベルづけしても曖昧な領域はある
- 出現頻度の低い特徴量とみせかけの相関

### DLでの汎化の性質
ディープラーニングにおける汎化の性質は、ディープラーニングモデル自体とはほとんど関係なく、現実世界の情報の構造と大いに関係がある。

### 多様体仮説
実際の手書きの数字は 28*28 のありとあらゆる uint8 型の配列からなる親空間のごく一部を占めているだけ。しかも、この部分空間は親空間にランダムに点在するデータ点の集まりではなく、高度に構造化されている。

まず、有効な手書きの数字からなる部分空間は連続的。２つの**異なる**数字間を変化させた時、その一連の「中間画像」が存在するが、それらもかなり数字っぽく見えるだろう。

多様体仮説（manifold hypothesis）では、「すべての自然データはそれがエンコードされている高次元空間内の低次元多様体上にある」と仮定する。この仮定は万物の情報の構造に関する非常に強い名声。私たちが知る限り、これは正しい仮定であり、DLがうまくいく理由もそこにある。

多様体仮説は２つのことを示唆する。

- 機械学習モデルは、潜在的な入力空間内の比較的単純で、低次元の、高度の構造化された部分空間（潜在多様体）に適合させるだけでよい
- これらの多様体の１つの中で、２つの入力の間の補間（interpolate）が常に可能である。つまり、一方の入力を連続的なパスでもう一方の入力に変形させることができる。そのパス沿いにある点は全てその多様体の上にある。

### 汎化のソースとしての補間
DLの汎化は学習によるデータ多様体の矜持に基づく保管によって実現されるものの、補間すれば汎化できると仮定するのは間違い！補間は氷山の一角にすぎない。

人間は極端な汎化を行う能力がある。このような汎化は（補間ではなく）認知メカニズム（いわゆる論理的思考）によって可能となる。

### DLの仕組み
- ディープラーニングモデルは入力値から出力値への滑らかで連続したマッピングを実装する。美文可能でなければならないため、必然的に滑らかで連続したものになる。この滑らかさが、同じ特性に従う潜在多様体の近似に役立つ
- ディープラーニングモデルは訓練データに含まれている情報の「形状」を忠実に再現するような方法で構造化される傾向にある。

### 最も重要なのは訓練データ
DLは確かに多様体の学習に適しているが、汎化する能力はモデルの特性というよりもデータの自然な構造によってもたらされるもの。汎化が可能になるのは、データが形成している多様性のデータ点を保管できる場合のみ。特徴量の情報利得が大きく、ノイズが少ないほど、入力空間がより単純になり、うまく構造化されるため、モデルがうまく汎化できるようになる。このため、汎化にはデータキュレーションと特徴量エンジニアリングが不可欠。


### モデルを評価する
情報の漏れを防ぐために、訓練データ、検証データ、テストデータの３つに分ける。

### モデルの評価に関する注意点
- データの典型性
- 時間の矢
  - 過去に基づいて未来を予測する場合、データを分割前にランダムシャッフルはできない。そうすれば時間の漏れを作ってしまう
- データの冗長性

### モデルのキャパシティを増やす
モデルを何とか適合させ、検証データの損失値が小さくなり、少なくともそれなりの汎化性能があるように見えたら、後一息。次はこのモデルを過学習させる必要がある。

モデルを過学習させることが常に可能でなければならない！！

### データセットのキュレーション
DLは曲線の近似であって、魔法ではない、

- 十分なデータがあることを確認。入力空間と出力空間の密なサンプリングが必要である。データが多いほどよいモデルになる
- ラベル付けの誤りをできるだけ減らす。異常を確認するために入力を可視化し、ラベルに誤りがないことをチェックする
- データを整理して欠損値に対処する
- 特徴量の数が多く、実際にどれが有益なのか分からない場合は、特徴量選択を行う。

### 特徴量エンジニアリング（feature engineering）
データと機械学習アルゴリズムについてのあなた自身の知識をもとに、そのアルゴリズムの性能を向上させるプロセス。データがモデルに渡される前に、ハードコーディングされた変換をデータに適応する。多くの場合、機械学習モデルが完全に任意のデータから学習できると想定するのは妥当ではない。

### 重みを正則化する
オッカムの剃刀（Occam's razor）。何かに対する説明が２つあるとしたら、正しいと思われるのは単純な方の説明、つまり、仮定が少ない方の説明。

単純なモデルとは、パラメータの値の分布に関するエントロピーが小さいモデルのこと。重みの値を制限することで、重みの値の分布がより正則化される。

重みを正則化するには、大きな重みを使うときのコストをモデルの損失関数に追加する。

- L1 正則化
- L2 正則化

重みの正則化はより小さなディープラーニングモデルで使うのが一般的

### ドロップアウトを追加する
NNで最も効果的で最もよく使われている正則化手法の１つ。ドロップアウト率は通常 0.2-0.5 で設定される。テストの際には、どのユニットにもドロップアウトは適用されず、代わりにその層の出力値がドロップアウト率と同じ割合でスケールダウンされる。

層の出力値にノイズを追加すると、重要ではない偶然のパターンを破壊できる。

### NNの汎化性能を最大化し、過学習を防ぐ方法まとめ
- 訓練データを増やす、訓練データの質を高める
- 特徴量の質を高める
- モデルのキャパシティを減らす
- （小さなモデルの）重みを正則化する
- ドロップアウトを追加する


## sec 6

### 倫理
テクノロジは決して中立的ではない。社会に影響を与えるような仕事についているとしたら、その影響には倫理の方向性がある。

### MLのユニバーサルワークフロー
1. タスクを定義する
2. モデルを開発する
3. モデルをデプロイする

### 代表的なサンプルではないデータに注意する
- サンプリングバイアス
- コンセプトドリフト
  - ユーザーが生成したデータを扱う問題で特に
  - 本番環境のデータの性質が次第に変化し、モデルの正解率が徐々に低下するなど

### 推論モデルの最適化
モデルをデプロイする環境に電力やメモリに関する厳しい制約がある、あるいはアプリケーションに低遅延要件がある場合は、モデルを推論のために最適化することが特に重要となる。モデルを TensorFlow.js にインポートしたり、TensorFlowLite にエクスポートしたりする前に、モデルを最適化する。

- 重みの刈り込み（weight pruning）
- 重みの量子化


## sec 7

### Keras のモデル
構築には次の３つの方法がある

- Sequential モデル
  - 最もアプローチしやすい
- Functional API
  - グラフ形式のモデルアーキテクチャに焦点を合わせている
- モデルのサブクラス化
  - 全てを１から記述する低レベルのAPI
  - 細かな部分まで何もかも完全に制御したい場合に最適

#### Functional API
Sequential モデルは使いやすいAPIだが、応用範囲が非常に限られる。このモデルを使って表現できるのは、入力と出力がそれぞれちょうど１つだけで、層から層へと逐次的に適応されるモデルだけ。実際には、複数の入力を持つモデル（画像とメタデータなど）、複数の出力を持つモデル（データについて複数のことを予測）、あるいは非線形のトポロジを使うモデルも割とよく使われる。

そのような場合は、Functional APIを使うことになる。

### Callback
EarlyStopping, ModelCheckpoint 等を、fit() の callbacks 引数で渡す

TensorBoard コールバック

### tf.function による高速化
カスタムループの実行では、Numpy や通常の Python のように eager 実行されるので効率的ではない。そこで、全体最適化を図るために評価ステップ関数に @tf.function デコレータをつ
ける

コードをデバッグするときにはデコレータは外す。


## sec 8

### 畳み込み演算
全結合層と畳み込み層の根本的な違いは、全結合層が入力特徴量空間から全体的なパターンを学習するのに対し、畳み込み層が局所的なパターンを学習する点にある。

CNNの性質

- CNNが学習するパターンは平行移動不変
- CNNはパターンの空間的な階層を学習できる
  - １つめの畳み込み層はエッジなどの小さな局所的パターンを学習し、２つめの畳み込み層は１つめの畳み込み層の特徴量から作られたより大きなパターンを学習する、といった具合になる。

### max pooling
分類モデルにおいてストライドの代わりに特徴量マップのダウンサンプリングによく使われる。

ダウンサンプリングを行わない場合の問題

- 特徴量の空間階層の学習に貢献しない
  - レイヤーを重ねても、見れる範囲がゆっくりしか広がっていかない
- 最終的な特徴量マップが非常に大きい

### 訓練済みのモデルで特徴量抽出を行う
訓練済みのモデルが学習した表現に基づいて新しいサンプルから興味深い特徴量を抽出するという手法。

CNN は２つの部分で構成されていて、１つが一連のプーリング層と畳み込み層で、２つめの部分が全結合分類器。畳み込みベースで学習した表現は汎用性が高く、再利用しやすい可能性があるためここを利用させてもらう。


## sec 9
CNN のベストプラクティス！

### 画像セグメンテーション
- セマンティックセグメンテーション
  - それぞれのピクセルが「猫」などの意味的なカテゴリに個別に分類される
  - 画像が二匹含まれている場合、該当するピクセルは全て同じ「猫」という汎用的なカテゴリにマッピングされる
- インスタンスセグメンテーション
  - 猫１と猫２を別々に扱う

このモデルでは、畳み込み層に交互にストライドを追加するという方法で、ダウンサンプリングを行なっている。画像セグメンテーションでは、モデルの出力としてピクセルごとに目的地（マスク）を生成する必要があるため、画像内の情報の**空間位置**が非常に重要になるから。

### CNNアーキテクチャパターン
レイヤーの選択により、モデルの**仮説空間**が定義される。仮説空間とは、勾配降下法で探索できる関数の空間のこと。良い仮説空間は与えられた問題とその解についての事前知識をエンコードする。たとえば、畳み込み層を使うということは、入力画像に存在しているパターンが並行移動不変であることが事前にわかっているということ！データから効果的に学習するには、探しているものについて仮説を立てる必要がある。

MHR（Modularity-Hierarchy-Reuse）

### モジュール化、階層化、再利用
モジュールにまとめ、階層にまとめ、再利用する。

### 残差接続
勾配消失を気にすることなく、どこまでも深いネットワークを構築するためのテクニック。

ノイズの乗らない、１つ（？）前の結果を残しておいて、加えてあげる

### バッチ正則化
バッチ正則化の主な効果は（残差接続と同様に）勾配の伝播を助けることにある。
なぜうまく働くかは分かっていないらしい。

### dws 畳み込み
深さ方向に分離可能な畳み込み層（depthwise separable convolution layer）。

Keras では SeparableConv2D として実装されている。ここでの仮定は、「中間層の活性化では空間位置に強い相関が認められるものの、それぞれのチャネルは独立性が高い」というもの。DLが学習する画像表現では、この過程が大体成り立つ。

GPUで実装されているのは「単純な」CUDA実装ではなく、cuDNNカーネルであるため、必ずしも高速化にはつながらない。それでも、パラメータの数が少なくなればそれだけ過学習のリスクが少なくなるため、使うべき！また、dws 畳み込みは「チャネルには相関がないはずだ」と仮定するため、モデルの収束が早まり、より堅牢な表現になる。

RGB 画像の場合には、チャネル間の相関がないという仮定は成り立たないので、最初のみはConv2Dで行う。

### 解釈
層によって抽出された特徴量が、そうが深くなるほど抽象的になっていく。

モデルの解釈可能性。CAM（Class Activation Map）


## sec 10
時系列データ：RNN

### 時系列タスク
- 予測
- 分類
- イベント検出
- 異常検知

### 気温を予測する例
データの周期性を常に調べる。

データの最初を訓練、次のデータのいくつかを検証、残りをテストに使うようにする！時系列データを扱うときには、検証データとテストデータが訓練データよりも直近のものであることが重要となる。

### RNN
RNNによるシーケンスの処理は、シーケンスの要素を反復的に処理しながら、その過程で見たものに関連する情報を状態（state）として維持するという方法で行われる。実質的には、RNNは内部ループを持つNNの一種。

### LSTM
LSTMせるのアーキテクチャを何もかも理解する必要はない（それは人間の仕事ではない）。LSTMセルは過去の情報を後から再注入できるようにすることで勾配消失問題に対応している、ということだけ覚えておく！

### GRU
Gated Reccurent Unit, LSTMアーキテクチャを少し単純化して効率化したもの

### 双方向のRNN
bidirectional RNN、テキストデータには最適。

GRUやLSTMといった標準的なRNNを２つ使う。これらのRNNはそれぞれ入力シーケンスの一方行の処理を行い、最後にそれぞれの表現をマージする。双方向RNNでは入力シーケンスを双方向で処理することで、単方向のRNNでは見逃してしまうかもしれないパターンを捕捉できる。

### さらに
> DLはやはり科学というよりも選択術。理論は存在せず、実践あるのみ。


## sec 11

### 自然言語処理
コンピュータ言語と区別するために「自然」言語と呼んでいる。コンピュータ言語は初めに「ルール」ありきで作られているが、自然言語は初めに「使用」ありきで後からルールが決まっている。自然言語の発展の仕方は生物と同じであり、そういう意味で「自然」といっている。

### テキストの前処理
DLのモデルは微分可能な関数であり、このモデルが処理できるのは数値のテンソルだけ。入力としてテキストをそのまま受け取れはしない。テキストを数値テンソルに変換することを、テキストのベクトル化と呼ぶ。

1. テキストを小文字に変換する、句読点を取り除くといった**標準化**を行う
2. テキストを文字、単語、単語のグループといったユニットに分解する（**トークン化**）
3. それらのトークンをそれぞれ数値ベクトルに変換する。通常は、その前に全てのトークンを**インデックス化**する

### インデックス化
インデックス0のマスクトークンと、インデックス１の OOV（Out of vocabulary）トークン

> TextVectorization の処理は主にディ区署なりの検索であるため、GPU（またはTPU）では実行することができず、CPUで実行するしかない

### TF-IDF 正規化
``` python
def tfidf(term, document, dataset):
    term_freq = document.count(term)
    doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)
    return term_freq / doc_freq
```

TF-IDF は非常によく使われるため、TextVectorization 層に組み込まれている。

### Self-Attention
モデルは一部の特徴量に「特別な注意をはらい」、他の特徴量に関しては「あまり注意を払わないようにする」べきである。実は、同様の概念は以下でも使われていた

- CNN の最大値プーリング。空間領域で特徴量のプールを調べて、残すべき特徴量を１つだけ選択
- TF-IDF 正規化。特徴量が運んでいると思われる情報の量に基づいてトークンに重要度を割り当て

スマートな埋め込み空間では、当然ながら、単語のベクトル表現はその周辺にある単語に応じて異なるものとなる。そこで Self-Attention が登場してくる。Self-Attention の目的は、シーケンス内の関連するトークンの表現をもとに、現在のトークンの表現を調整すること。

### シーケンスモデルと BoW モデルの使い分け
- サンプルの数 / サンプルの平均的な長さ -> 1500 より大きい場合
  - Transformer ベースのシーケンスモデル
- サンプルの数 / サンプルの平均的な長さ -> 1500 より小さい場合
  - Bag-of-bigrams モデル

シーケンスモデルの入力の方が表現豊かで複雑な空間を表すため、その空間を描画するのにより多くのデータが必要になる！

### Sequence-to-Sequence
- 機械翻訳
- テキスト要約
- 質疑応答
- チャットボット
- テキスト生成

### RNN を使った Sequence-to-Sequence
2017年ごろの Google Translate のエンジンは、７つの大きな LSTM 層からなるスタックだった。

RNN は過去のことを徐々に忘れていく傾向にあるため、非常に長い文にうまく

### まとめ
- BoW
  - 単語の順番を考慮しないNグラム
  - Dense層で構成
- シーケンスモデル
  - 単語の順番を考慮する
  - RNN, 1d CNN, Transform のいずれかを利用可


## sec 12
生成型 DL

### シーケンスデータ
最初の成功例は RNN, その後生成型 Transformer として GPT-3。

前のトークンから次のトークンの確率を予測できるモデルを言語モデル（language model）と呼ぶ。

``` python
import numpy as np

def reweight_distribution(original_distribution, temperature=0.5):
    distribution = np.log(original_distribution) / temperature
    distribution = np.exp(distribution)
    return distribution / np.sum(distribution)
```

### DeepDream
CNN が学習した表現を使う芸術的な画像加工手法

- DeepDream では、特定のフィルタではなく層全体の活性化を最大化する。
- 出発点が（ノイズが少し含まれた）からの入力ではなく既存の画像であるため、結果として得られた効果が既存の視覚的パターンとしっかり結びつき、画像の要素をどことなく芸術的な方法で歪ませる。
- 入力画像はさまざまな尺度で処理されるため、可視化の品質が良くなる。これらの尺度をオクターブ（octave）と呼ぶ。
- Keras での訓練済みの CNN
  - VGG16
  - VGG19
  - Xception
  - ResNet50
  - etc.
- 下位にある層ほど幾何学的なパターンを生成
- 上位にある層ほど ImageNet のクラス（鳥、犬など）が見分けられるような表現を生成
- DeepDream は、CNN を逆向きに実行することで、ネットワークが学習した表現に基づいて入力を生成する仕組み
- 結果として、まるで幻覚を見ているような面白い画像が生成される

### NN によるスタイル変換
ニューラルスタイル変換（neural style transfer）。ベース画像の内容を維持した上で、リファレンス画像のスタイルをベース画像に適応するという仕組み。

ここでのスタイルとは、基本的に、さまざまな空間的尺度での画像のテクスチャ、色、視覚パターンを意味する。そして内容とは、画像の俯瞰的なマクロ構造のこと。

何を達成したいのかを指定する損失関数さえ定義したら良い！

- スタイルの損失関数
  - グラム行列（Gram matrix）
    - 与えられた層の特徴量マップ同士の内積
- 内容の損失関数
  - 


