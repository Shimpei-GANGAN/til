## sec 2
誤差伝播方は連鎖率を計算グラフに適応したものである。

Tensorflow の強力な自動美文機能を活用できるAPIはGradientTape。GradientTapeはPythonのスコープで、その中で実行されるテンソル演算を計算グラフとして記録する。

## sec 3

### TensorFlow, Keras
TensorFlowはPythonベースのオープンソースのMLプラットフォームであり、主にGoogleによって開発されている。

KerasはTensorFlowをベースに構築されたPython用のディープラーニングAPI。Kerasはユーザーエクスペリエンスを一番に考えているらしい。Kerasはマシンではなく人間のためのAPIであり、認知的負荷を減らすためにベストプラクティスに従っている。つまり、一貫性のあるワークフローを提供し、一般的なユースケースで必要となるアクションの数を最小限に抑え、ユーザーエラーに対して明確ですぐに行動に移せるフィードバックを提供する。

### 環境
ローカルとクラウドのどちらで実行する場合でも、Unixワークステーションを使うほうがよいでしょう


## sec 4

### 二値分類の例
交差エントロピーとは、２つの確率分布の間の距離を表す情報理論の尺度。

### 回帰（Regression）
テストデータの正規化に使われる値は、訓練データを使って計算する。
たとえデータの正規化のような単純なものであっても、ワークフローでは決してテストデータを使って計算した値を使うべきではない。

小さいモデルを使うことは過学習を抑制する方法の１つ

### k分割交差検証
利用できるデータが少ない場合、モデルを正確に評価するのに k分割交差検証を使う。
k-fold cross-validation でパラメータを決定した後は、全データを使って訓練し直す。


## sec 5
機械学習の根本的な課題は、最適化と汎化の間の緊張関係にある。

最適化とは、訓練データの性能ができるだけ良くなるようモデルを調整するプロセスのことで、機械学習の学習にあたる。汎化（generalizatioin）とは、訓練済みのモデルを未知のデータで実行したときの性能がどれくらい良いかを表す。

汎化性能を向上させることが機械学習の目標であるが、汎化は制御できない。

### 過学習を起こしやすいもの
- ノイズを含んでいる訓練データ
  - 自然のデータにはノイズがのる
- 曖昧な特徴量
  - 人間がラベルづけしても曖昧な領域はある
- 出現頻度の低い特徴量とみせかけの相関

### DLでの汎化の性質
ディープラーニングにおける汎化の性質は、ディープラーニングモデル自体とはほとんど関係なく、現実世界の情報の構造と大いに関係がある。

### 多様体仮説
実際の手書きの数字は 28*28 のありとあらゆる uint8 型の配列からなる親空間のごく一部を占めているだけ。しかも、この部分空間は親空間にランダムに点在するデータ点の集まりではなく、高度に構造化されている。

まず、有効な手書きの数字からなる部分空間は連続的。２つの**異なる**数字間を変化させた時、その一連の「中間画像」が存在するが、それらもかなり数字っぽく見えるだろう。

多様体仮説（manifold hypothesis）では、「すべての自然データはそれがエンコードされている高次元空間内の低次元多様体上にある」と仮定する。この仮定は万物の情報の構造に関する非常に強い名声。私たちが知る限り、これは正しい仮定であり、DLがうまくいく理由もそこにある。

多様体仮説は２つのことを示唆する。

- 機械学習モデルは、潜在的な入力空間内の比較的単純で、低次元の、高度の構造化された部分空間（潜在多様体）に適合させるだけでよい
- これらの多様体の１つの中で、２つの入力の間の補間（interpolate）が常に可能である。つまり、一方の入力を連続的なパスでもう一方の入力に変形させることができる。そのパス沿いにある点は全てその多様体の上にある。

### 汎化のソースとしての補間
DLの汎化は学習によるデータ多様体の矜持に基づく保管によって実現されるものの、補間すれば汎化できると仮定するのは間違い！補間は氷山の一角にすぎない。

人間は極端な汎化を行う能力がある。このような汎化は（補間ではなく）認知メカニズム（いわゆる論理的思考）によって可能となる。

### DLの仕組み
- ディープラーニングモデルは入力値から出力値への滑らかで連続したマッピングを実装する。美文可能でなければならないため、必然的に滑らかで連続したものになる。この滑らかさが、同じ特性に従う潜在多様体の近似に役立つ
- ディープラーニングモデルは訓練データに含まれている情報の「形状」を忠実に再現するような方法で構造化される傾向にある。

### 最も重要なのは訓練データ
DLは確かに多様体の学習に適しているが、汎化する能力はモデルの特性というよりもデータの自然な構造によってもたらされるもの。汎化が可能になるのは、データが形成している多様性のデータ点を保管できる場合のみ。特徴量の情報利得が大きく、ノイズが少ないほど、入力空間がより単純になり、うまく構造化されるため、モデルがうまく汎化できるようになる。このため、汎化にはデータキュレーションと特徴量エンジニアリングが不可欠。


### モデルを評価する
情報の漏れを防ぐために、訓練データ、検証データ、テストデータの３つに分ける。

### モデルの評価に関する注意点
- データの典型性
- 時間の矢
  - 過去に基づいて未来を予測する場合、データを分割前にランダムシャッフルはできない。そうすれば時間の漏れを作ってしまう
- データの冗長性

### モデルのキャパシティを増やす
モデルを何とか適合させ、検証データの損失値が小さくなり、少なくともそれなりの汎化性能があるように見えたら、後一息。次はこのモデルを過学習させる必要がある。

モデルを過学習させることが常に可能でなければならない！！

### データセットのキュレーション
DLは曲線の近似であって、魔法ではない、

- 十分なデータがあることを確認。入力空間と出力空間の密なサンプリングが必要である。データが多いほどよいモデルになる
- ラベル付けの誤りをできるだけ減らす。異常を確認するために入力を可視化し、ラベルに誤りがないことをチェックする
- データを整理して欠損値に対処する
- 特徴量の数が多く、実際にどれが有益なのか分からない場合は、特徴量選択を行う。

### 特徴量エンジニアリング（feature engineering）
データと機械学習アルゴリズムについてのあなた自身の知識をもとに、そのアルゴリズムの性能を向上させるプロセス。データがモデルに渡される前に、ハードコーディングされた変換をデータに適応する。多くの場合、機械学習モデルが完全に任意のデータから学習できると想定するのは妥当ではない。

### 重みを正則化する
オッカムの剃刀（Occam's razor）。何かに対する説明が２つあるとしたら、正しいと思われるのは単純な方の説明、つまり、仮定が少ない方の説明。

単純なモデルとは、パラメータの値の分布に関するエントロピーが小さいモデルのこと。重みの値を制限することで、重みの値の分布がより正則化される。

重みを正則化するには、大きな重みを使うときのコストをモデルの損失関数に追加する。

- L1 正則化
- L2 正則化

重みの正則化はより小さなディープラーニングモデルで使うのが一般的

### ドロップアウトを追加する
NNで最も効果的で最もよく使われている正則化手法の１つ。ドロップアウト率は通常 0.2-0.5 で設定される。テストの際には、どのユニットにもドロップアウトは適用されず、代わりにその層の出力値がドロップアウト率と同じ割合でスケールダウンされる。

層の出力値にノイズを追加すると、重要ではない偶然のパターンを破壊できる。

### NNの汎化性能を最大化し、過学習を防ぐ方法まとめ
- 訓練データを増やす、訓練データの質を高める
- 特徴量の質を高める
- モデルのキャパシティを減らす
- （小さなモデルの）重みを正則化する
- ドロップアウトを追加する


## sec 6

### 倫理
テクノロジは決して中立的ではない。社会に影響を与えるような仕事についているとしたら、その影響には倫理の方向性がある。

### MLのユニバーサルワークフロー
1. タスクを定義する
2. モデルを開発する
3. モデルをデプロイする

### 代表的なサンプルではないデータに注意する
- サンプリングバイアス
- コンセプトドリフト
  - ユーザーが生成したデータを扱う問題で特に
  - 本番環境のデータの性質が次第に変化し、モデルの正解率が徐々に低下するなど

### 推論モデルの最適化
モデルをデプロイする環境に電力やメモリに関する厳しい制約がある、あるいはアプリケーションに低遅延要件がある場合は、モデルを推論のために最適化することが特に重要となる。モデルを TensorFlow.js にインポートしたり、TensorFlowLite にエクスポートしたりする前に、モデルを最適化する。

- 重みの刈り込み（weight pruning）
- 重みの量子化



