## sec 2
誤差伝播方は連鎖率を計算グラフに適応したものである。

Tensorflow の強力な自動美文機能を活用できるAPIはGradientTape。GradientTapeはPythonのスコープで、その中で実行されるテンソル演算を計算グラフとして記録する。

## sec 3

### TensorFlow, Keras
TensorFlowはPythonベースのオープンソースのMLプラットフォームであり、主にGoogleによって開発されている。

KerasはTensorFlowをベースに構築されたPython用のディープラーニングAPI。Kerasはユーザーエクスペリエンスを一番に考えているらしい。Kerasはマシンではなく人間のためのAPIであり、認知的負荷を減らすためにベストプラクティスに従っている。つまり、一貫性のあるワークフローを提供し、一般的なユースケースで必要となるアクションの数を最小限に抑え、ユーザーエラーに対して明確ですぐに行動に移せるフィードバックを提供する。

### 環境
ローカルとクラウドのどちらで実行する場合でも、Unixワークステーションを使うほうがよいでしょう


## sec 4

### 二値分類の例
交差エントロピーとは、２つの確率分布の間の距離を表す情報理論の尺度。

### 回帰（Regression）
テストデータの正規化に使われる値は、訓練データを使って計算する。
たとえデータの正規化のような単純なものであっても、ワークフローでは決してテストデータを使って計算した値を使うべきではない。

小さいモデルを使うことは過学習を抑制する方法の１つ

### k分割交差検証
利用できるデータが少ない場合、モデルを正確に評価するのに k分割交差検証を使う。
k-fold cross-validation でパラメータを決定した後は、全データを使って訓練し直す。


## sec 5
機械学習の根本的な課題は、最適化と汎化の間の緊張関係にある。

最適化とは、訓練データの性能ができるだけ良くなるようモデルを調整するプロセスのことで、機械学習の学習にあたる。汎化（generalizatioin）とは、訓練済みのモデルを未知のデータで実行したときの性能がどれくらい良いかを表す。

汎化性能を向上させることが機械学習の目標であるが、汎化は制御できない。

### 過学習を起こしやすいもの
- ノイズを含んでいる訓練データ
  - 自然のデータにはノイズがのる
- 曖昧な特徴量
  - 人間がラベルづけしても曖昧な領域はある
- 出現頻度の低い特徴量とみせかけの相関

### DLでの汎化の性質
ディープラーニングにおける汎化の性質は、ディープラーニングモデル自体とはほとんど関係なく、現実世界の情報の構造と大いに関係がある。

### 多様体仮説
実際の手書きの数字は 28*28 のありとあらゆる uint8 型の配列からなる親空間のごく一部を占めているだけ。しかも、この部分空間は親空間にランダムに点在するデータ点の集まりではなく、高度に構造化されている。

まず、有効な手書きの数字からなる部分空間は連続的。２つの**異なる**数字間を変化させた時、その一連の「中間画像」が存在するが、それらもかなり数字っぽく見えるだろう。

多様体仮説（manifold hypothesis）では、「すべての自然データはそれがエンコードされている高次元空間内の低次元多様体上にある」と仮定する。この仮定は万物の情報の構造に関する非常に強い名声。私たちが知る限り、これは正しい仮定であり、DLがうまくいく理由もそこにある。

多様体仮説は２つのことを示唆する。

- 機械学習モデルは、潜在的な入力空間内の比較的単純で、低次元の、高度の構造化された部分空間（潜在多様体）に適合させるだけでよい
- これらの多様体の１つの中で、２つの入力の間の補間（interpolate）が常に可能である。つまり、一方の入力を連続的なパスでもう一方の入力に変形させることができる。そのパス沿いにある点は全てその多様体の上にある。

### 汎化のソースとしての補間
DLの汎化は学習によるデータ多様体の矜持に基づく保管によって実現されるものの、補間すれば汎化できると仮定するのは間違い！補間は氷山の一角にすぎない。

人間は極端な汎化を行う能力がある。このような汎化は（補間ではなく）認知メカニズム（いわゆる論理的思考）によって可能となる。

### DLの仕組み
- ディープラーニングモデルは入力値から出力値への滑らかで連続したマッピングを実装する。美文可能でなければならないため、必然的に滑らかで連続したものになる。この滑らかさが、同じ特性に従う潜在多様体の近似に役立つ
- ディープラーニングモデルは訓練データに含まれている情報の「形状」を忠実に再現するような方法で構造化される傾向にある。

### 最も重要なのは訓練データ
DLは確かに多様体の学習に適しているが、汎化する能力はモデルの特性というよりもデータの自然な構造によってもたらされるもの。汎化が可能になるのは、データが形成している多様性のデータ点を保管できる場合のみ。特徴量の情報利得が大きく、ノイズが少ないほど、入力空間がより単純になり、うまく構造化されるため、モデルがうまく汎化できるようになる。このため、汎化にはデータキュレーションと特徴量エンジニアリングが不可欠。


### モデルを評価する
情報の漏れを防ぐために、訓練データ、検証データ、テストデータの３つに分ける。

### モデルの評価に関する注意点
- データの典型性
- 時間の矢
  - 過去に基づいて未来を予測する場合、データを分割前にランダムシャッフルはできない。そうすれば時間の漏れを作ってしまう
- データの冗長性

### モデルのキャパシティを増やす
モデルを何とか適合させ、検証データの損失値が小さくなり、少なくともそれなりの汎化性能があるように見えたら、後一息。次はこのモデルを過学習させる必要がある。

モデルを過学習させることが常に可能でなければならない！！

### データセットのキュレーション
DLは曲線の近似であって、魔法ではない、

- 十分なデータがあることを確認。入力空間と出力空間の密なサンプリングが必要である。データが多いほどよいモデルになる
- ラベル付けの誤りをできるだけ減らす。異常を確認するために入力を可視化し、ラベルに誤りがないことをチェックする
- データを整理して欠損値に対処する
- 特徴量の数が多く、実際にどれが有益なのか分からない場合は、特徴量選択を行う。

### 特徴量エンジニアリング（feature engineering）
データと機械学習アルゴリズムについてのあなた自身の知識をもとに、そのアルゴリズムの性能を向上させるプロセス。データがモデルに渡される前に、ハードコーディングされた変換をデータに適応する。多くの場合、機械学習モデルが完全に任意のデータから学習できると想定するのは妥当ではない。

### 重みを正則化する
オッカムの剃刀（Occam's razor）。何かに対する説明が２つあるとしたら、正しいと思われるのは単純な方の説明、つまり、仮定が少ない方の説明。

単純なモデルとは、パラメータの値の分布に関するエントロピーが小さいモデルのこと。重みの値を制限することで、重みの値の分布がより正則化される。

重みを正則化するには、大きな重みを使うときのコストをモデルの損失関数に追加する。

- L1 正則化
- L2 正則化

重みの正則化はより小さなディープラーニングモデルで使うのが一般的

### ドロップアウトを追加する
NNで最も効果的で最もよく使われている正則化手法の１つ。ドロップアウト率は通常 0.2-0.5 で設定される。テストの際には、どのユニットにもドロップアウトは適用されず、代わりにその層の出力値がドロップアウト率と同じ割合でスケールダウンされる。

層の出力値にノイズを追加すると、重要ではない偶然のパターンを破壊できる。

### NNの汎化性能を最大化し、過学習を防ぐ方法まとめ
- 訓練データを増やす、訓練データの質を高める
- 特徴量の質を高める
- モデルのキャパシティを減らす
- （小さなモデルの）重みを正則化する
- ドロップアウトを追加する


## sec 6

### 倫理
テクノロジは決して中立的ではない。社会に影響を与えるような仕事についているとしたら、その影響には倫理の方向性がある。

### MLのユニバーサルワークフロー
1. タスクを定義する
2. モデルを開発する
3. モデルをデプロイする

### 代表的なサンプルではないデータに注意する
- サンプリングバイアス
- コンセプトドリフト
  - ユーザーが生成したデータを扱う問題で特に
  - 本番環境のデータの性質が次第に変化し、モデルの正解率が徐々に低下するなど

### 推論モデルの最適化
モデルをデプロイする環境に電力やメモリに関する厳しい制約がある、あるいはアプリケーションに低遅延要件がある場合は、モデルを推論のために最適化することが特に重要となる。モデルを TensorFlow.js にインポートしたり、TensorFlowLite にエクスポートしたりする前に、モデルを最適化する。

- 重みの刈り込み（weight pruning）
- 重みの量子化


## sec 7

### Keras のモデル
構築には次の３つの方法がある

- Sequential モデル
  - 最もアプローチしやすい
- Functional API
  - グラフ形式のモデルアーキテクチャに焦点を合わせている
- モデルのサブクラス化
  - 全てを１から記述する低レベルのAPI
  - 細かな部分まで何もかも完全に制御したい場合に最適

#### Functional API
Sequential モデルは使いやすいAPIだが、応用範囲が非常に限られる。このモデルを使って表現できるのは、入力と出力がそれぞれちょうど１つだけで、層から層へと逐次的に適応されるモデルだけ。実際には、複数の入力を持つモデル（画像とメタデータなど）、複数の出力を持つモデル（データについて複数のことを予測）、あるいは非線形のトポロジを使うモデルも割とよく使われる。

そのような場合は、Functional APIを使うことになる。

### Callback
EarlyStopping, ModelCheckpoint 等を、fit() の callbacks 引数で渡す

TensorBoard コールバック

### tf.function による高速化
カスタムループの実行では、Numpy や通常の Python のように eager 実行されるので効率的ではない。そこで、全体最適化を図るために評価ステップ関数に @tf.function デコレータをつ
ける

コードをデバッグするときにはデコレータは外す。


## sec 8

### 畳み込み演算
全結合層と畳み込み層の根本的な違いは、全結合層が入力特徴量空間から全体的なパターンを学習するのに対し、畳み込み層が局所的なパターンを学習する点にある。

CNNの性質

- CNNが学習するパターンは平行移動不変
- CNNはパターンの空間的な階層を学習できる
  - １つめの畳み込み層はエッジなどの小さな局所的パターンを学習し、２つめの畳み込み層は１つめの畳み込み層の特徴量から作られたより大きなパターンを学習する、といった具合になる。

### max pooling
分類モデルにおいてストライドの代わりに特徴量マップのダウンサンプリングによく使われる。

ダウンサンプリングを行わない場合の問題

- 特徴量の空間階層の学習に貢献しない
  - レイヤーを重ねても、見れる範囲がゆっくりしか広がっていかない
- 最終的な特徴量マップが非常に大きい

### 訓練済みのモデルで特徴量抽出を行う
訓練済みのモデルが学習した表現に基づいて新しいサンプルから興味深い特徴量を抽出するという手法。

CNN は２つの部分で構成されていて、１つが一連のプーリング層と畳み込み層で、２つめの部分が全結合分類器。畳み込みベースで学習した表現は汎用性が高く、再利用しやすい可能性があるためここを利用させてもらう。


## sec 9
CNN のベストプラクティス！

### 画像セグメンテーション
- セマンティックセグメンテーション
  - それぞれのピクセルが「猫」などの意味的なカテゴリに個別に分類される
  - 画像が二匹含まれている場合、該当するピクセルは全て同じ「猫」という汎用的なカテゴリにマッピングされる
- インスタンスセグメンテーション
  - 猫１と猫２を別々に扱う

このモデルでは、畳み込み層に交互にストライドを追加するという方法で、ダウンサンプリングを行なっている。画像セグメンテーションでは、モデルの出力としてピクセルごとに目的地（マスク）を生成する必要があるため、画像内の情報の**空間位置**が非常に重要になるから。

### CNNアーキテクチャパターン
レイヤーの選択により、モデルの**仮説空間**が定義される。仮説空間とは、勾配降下法で探索できる関数の空間のこと。良い仮説空間は与えられた問題とその解についての事前知識をエンコードする。たとえば、畳み込み層を使うということは、入力画像に存在しているパターンが並行移動不変であることが事前にわかっているということ！データから効果的に学習するには、探しているものについて仮説を立てる必要がある。

MHR（Modularity-Hierarchy-Reuse）

### モジュール化、階層化、再利用
モジュールにまとめ、階層にまとめ、再利用する。

### 残差接続
勾配消失を気にすることなく、どこまでも深いネットワークを構築するためのテクニック。

ノイズの乗らない、１つ（？）前の結果を残しておいて、加えてあげる

### バッチ正則化
バッチ正則化の主な効果は（残差接続と同様に）勾配の伝播を助けることにある。
なぜうまく働くかは分かっていないらしい。

### dws 畳み込み
深さ方向に分離可能な畳み込み層（depthwise separable convolution layer）。

Keras では SeparableConv2D として実装されている。ここでの仮定は、「中間層の活性化では空間位置に強い相関が認められるものの、それぞれのチャネルは独立性が高い」というもの。DLが学習する画像表現では、この過程が大体成り立つ。

GPUで実装されているのは「単純な」CUDA実装ではなく、cuDNNカーネルであるため、必ずしも高速化にはつながらない。それでも、パラメータの数が少なくなればそれだけ過学習のリスクが少なくなるため、使うべき！また、dws 畳み込みは「チャネルには相関がないはずだ」と仮定するため、モデルの収束が早まり、より堅牢な表現になる。

RGB 画像の場合には、チャネル間の相関がないという仮定は成り立たないので、最初のみはConv2Dで行う。

### 解釈
層によって抽出された特徴量が、そうが深くなるほど抽象的になっていく。

モデルの解釈可能性。CAM（Class Activation Map）


