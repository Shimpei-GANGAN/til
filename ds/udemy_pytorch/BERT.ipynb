{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfor.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## IMDB Dataset\n",
        "- A large dataset of moview review comments\n",
        "- Reviews are flagged as positive or negative\n",
        "- Contains 25,000 training data and 25,000 test data"
      ],
      "metadata": {
        "id": "Zfgk83DhcKsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJFNS2eDcG_-",
        "outputId": "23edacb9-eff8-4cf7-b275-de57aedf1b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.1M/84.1M [00:03<00:00, 22.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchtext.legacy import data; # Field/Label\n",
        "from torchtext.legacy import datasets; # IMDB\n",
        "\n",
        "TEXT = data.Field(sequential=True, fix_length=256, batch_first=True, lower=True, tokenize=lambda s:s)\n",
        "LABEL = data.LabelField(sequential=True)\n",
        "\n",
        "train_all, test_all = datasets.IMDB.splits(TEXT, LABEL)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_all))\n",
        "print(len(test_all))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD02bxnVgSx5",
        "outputId": "c8a62850-656d-40e2-e56c-a4fb6dd1a8bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n",
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([t for t in train_all.text][0])\n",
        "print([t for t in train_all.label][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhjuggxKcsXC",
        "outputId": "e5227f77-11d5-426d-a596-b0723f848e7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i don't know whether to recommend this movie to the fans of \" tetsuo \" or not . why \" tetsuo \" ? because you can easily label some things about this movie as a very obvious \" tetsuo \" rip - off . the concept is similar , editing is equally frantic and fast - which is good because , aside from making the movie more dynamic , it obscures some flaws caused by low budget and other factors .<br /><br />there is lot more gore , less eroticism and , in the case of \" meatball machine \" , the transformation of human being into a creature that's partially a machine( sounds familiar ? ) called \" necroborg \" ( very original ) is caused by slimy little aliens .<br /><br />these slimy little scums from outer space actually use human beings as vessels for their gladiator games that they play with each other . they infest the body , somehow manage to put an insane amount of mechanical parts in it pulling them seemingly out of nowhere and turn it into a killing machine that targets other necroborgs . their aim is to defeat another alien who is in another necroborg , rip it out of the corpse and eat it .<br /><br />all in all , the plot sounds somewhat silly and i didn't expect much , but at the end i actually enjoyed this film .<br /><br />as i said before , this is a low budget flick , but it's still relatively decent . don't expect much from actors , they're mostly not very good , but it can be tolerated . i liked the atmosphere and gore , certain bizarre situations and the way the movie is directed and edited . although the story is not too original , it possesses certain charm - to me at least .<br /><br />7 out of 10 .\n",
            "pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Randomly Extract The Dataset"
      ],
      "metadata": {
        "id": "uMo1tykygmXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "Op0JS6WddXrn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 512\n",
        "test_size = 64"
      ],
      "metadata": {
        "id": "00aB_ZYEgomi"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_neg_dic = {\n",
        "    'neg': 0,\n",
        "    'pos': 1,\n",
        "}"
      ],
      "metadata": {
        "id": "jSnHsfJbg-cJ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = random.sample([(t, pos_neg_dic[l]) for (t, l) in zip(train_all.text, train_all.label)], train_size)"
      ],
      "metadata": {
        "id": "DoVIdkJngysw"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NORrZGhHhCy4",
        "outputId": "ed1010a4-fddb-4acf-9c96-b4050c65b832"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fabulous costumes by edith head who painted them on liz taylor at her finest!<br /><br />the sfx are very good for a movie of its age, and the stunt doubles actually looked like the actors, even down to body type, a rarity in movies of this vintage.<br /><br />a cozy movie, with splendid panoramas -- even when chopped down to pan and scan.',\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = random.sample([(t, pos_neg_dic[l]) for (t, l) in zip(test_all.text, test_all.label)], test_size)"
      ],
      "metadata": {
        "id": "q807DqbrhK_I"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pin6S4fqhZ8B",
        "outputId": "1811401c-246b-48f9-f3fe-0345844bdc08"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('**warning! slight plot spoilers ahead!**<br /><br />\"the italian job\" is not the best movie you\\'ll see all year, or probably even this summer. but it is a worthwhile two hours because it colors within the lines, knowing its limits and not attempting to exceed them.<br /><br />what carries the movie is the work of the cast. in a movie about a crew of thieves, the individuals must have a good rapport with each other. without that cohesive feel, the audience doesn\\'t believe in the characters collectively or individually, and the movie never has a chance. but from the first scenes, in which the men joke around and rag on each other while infiltrating a venetian palace, the proper chemistry is in place.<br /><br />the characters themselves aren\\'t anything novel; they\\'re your basic gang of criminals, containing about half a dozen players, each with a specific and defining skill. but each actor brings the proper goods to the table for his or her part. mark wahlberg\\'s understated acting and humor fits well with his part as the mastermind planner. edward norton provides attitude and twirls his mustache well in his dark role. donald sutherland is the father figure of the crew, and he looks the part of the suave and old-fashioned thief, who is still mentally spry. jason statham, seth green, and mos def don\\'t do much beyond their character\\'s abilities, but they each nail those parts. statham as the smooth-operating driver; green as the tech whiz geek with a chip on his shoulder; and def as the demolitions man. charlize theron slides in well in a part that doesn\\'t ask too much of her. she is primarily asked to to drive fast and look good. that she does. none of the characters are that deep or three-dimensional, but in this familiar sort of movie, two dimensions are all that is required. <br /><br />as the title implies, the movie has a european feel to it, a la \"the bourne identity,\" in part because it was shot on location in venice, along with philadelphia and los angeles. also contributing to the euro flair is the rhythmic, bouncy music, which adds to the upbeat nature of the flick and complements the rapport of the cast. the look of the movie is also a perfect match. the bright colors of all locales enhance the mood and add to the attitude. the minis not only provide a fun variation on the car chase, but also work as a necessary plot device. <br /><br />the plot is more or less straight-forward. there are a few surprises, but they are more of the swift-and-smooth-turn variety, as opposed to the drop-your-jaw hairpin curve. even with those, the movie speeds along. once the foundation is laid by the first act, everything continuously progresses. thankfully there are no breaks in the action for a romance, something the movie wisely avoided. there aren\\'t even any breaks for \\'real life.\\' the story has its purpose and runs that course without distractions. the lack of character depth prevents \"the italian job\" from being more than a good popcorn movie, but with all the complex details of the heist-planning, such superfluities would have dragged down the pace and quality of the flick.<br /><br />there are a number of implausibilities that i thought of both during and after viewing. but the movie is so enjoyable that i didn\\'t and don\\'t care. in the real world, most of the movie probably couldn\\'t have gone off that cleanly. but \"the italian job\" doesn\\'t take place in the real world. it occurs in a stylish and light-hearted criminal world that appeals to the rebel in all of us. <br /><br />\"the italian job\" is a movie, in the true sense of the word. it has no pretenses of oscar and contains no deep moral message. it provides pure escapism entertainment and does so quite well.<br /><br />bottom line: maybe the best popcorn movie of the year so far. 7 of 10.',\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.DataFrame(train, columns=['TEXT', 'LABEL'])\n",
        "test = pd.DataFrame(test, columns=['TEXT', 'LABEL'])"
      ],
      "metadata": {
        "id": "I0U_2l-bhdEs"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "E05wOQuGhvFN",
        "outputId": "7051305e-f190-4c46-ad8c-918bc4fea78d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-da154801-c7fb-4477-bb6e-1059091e0a78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>LABEL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fabulous costumes by edith head who painted th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i'll admit to being biased when i reviewed thi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the most moving and truly eye opening document...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>north africa in the 1930's. to a small arab to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>this is the best film the derek couple has eve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da154801-c7fb-4477-bb6e-1059091e0a78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da154801-c7fb-4477-bb6e-1059091e0a78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da154801-c7fb-4477-bb6e-1059091e0a78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                TEXT  LABEL\n",
              "0  fabulous costumes by edith head who painted th...      1\n",
              "1  i'll admit to being biased when i reviewed thi...      1\n",
              "2  the most moving and truly eye opening document...      1\n",
              "3  north africa in the 1930's. to a small arab to...      1\n",
              "4  this is the best film the derek couple has eve...      0"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['LABEL'].value_counts(normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQb0-EDrhvwF",
        "outputId": "f88e9b9a-74da-4b1e-d662-245bf4661b95"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.533203\n",
              "0    0.466797\n",
              "Name: LABEL, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==3.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VoYTSnFh7Jw",
        "outputId": "4f96c177-a47f-4826-ac46-10a4c6cabba1"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (3.4.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.1.96)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (21.3)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.8.1rc2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.0.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0) (3.0.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "4sueHrWXiRon"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "Mkm0L7E9igeo"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer('I like an apple')\n",
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YliT0_cMjLH9",
        "outputId": "463361c4-8bea-4d21-da23-cb2b098c524e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1045, 2066, 2019, 6207, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoded['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6oXqvpImjPQN",
        "outputId": "cdc7e9a5-f205-4cd9-967d-c98e7daa06a1"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] i like an apple [SEP]'"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer('I like an apple', 'I like an apple watch')\n",
        "print(encoded)\n",
        "print(tokenizer.decode(encoded['input_ids']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb3o7DPIjtfs",
        "outputId": "264553c6-d2eb-4828-fc4c-c7f50d86102f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 1045, 2066, 2019, 6207, 102, 1045, 2066, 2019, 6207, 3422, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "[CLS] i like an apple [SEP] i like an apple watch [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Histogram of tokenized text for deciding maximum length for the input"
      ],
      "metadata": {
        "id": "zuBhYkqrj7MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = [len(w.split()) for w in train['TEXT']]\n",
        "pd.Series(sequence_length).hist(bins=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "C5SvtOKljjGb",
        "outputId": "bd5122d1-4425-45dc-c017-977bd1fefc44"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff5703a2590>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 256"
      ],
      "metadata": {
        "id": "SP-ngTAikH2i"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train['TEXT'].tolist(),\n",
        "    max_length = max_sequence_length,\n",
        "    pad_to_max_length = True,\n",
        "    truncation = True,\n",
        "    return_token_type_ids = False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ4PrYkclEX3",
        "outputId": "50aa095b-6522-4cc0-c9c1-92acf40d02d8"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len([t for t in tokens_train['input_ids']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yphSrk6lUYT",
        "outputId": "1710eef8-63a6-4d88-e101-0cf6e65bf3fd"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test['TEXT'].tolist(),\n",
        "    max_length = max_sequence_length,\n",
        "    pad_to_max_length = True,\n",
        "    truncation = True,\n",
        "    return_token_type_ids = False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fuc2nyElYpe",
        "outputId": "1882d855-8f2a-4195-a985-81bdac28a74f"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "d4XYdj22mGU8"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train['LABEL'], dtype=torch.long)"
      ],
      "metadata": {
        "id": "Tvyhd7WulmNM"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test['LABEL'], dtype=torch.long)"
      ],
      "metadata": {
        "id": "DYHmQfozmBBW"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "metadata": {
        "id": "5wJ4LvnhmK1k"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16"
      ],
      "metadata": {
        "id": "fQ9_1rVXmdJ4"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Ya6IH1Ngmd-r"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "test_sampler = SequentialSampler(test_data)     # ここはランダムじゃない！\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "DBYTsxUQmvlA"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "bjE22yb6m5bC"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert = AutoModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "V3UX1hjgm-Xu"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "seEJ006UnH83"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "iw2Y2uOknVfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "Sya0ZHj1nMX8"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZQaChvuoCmr",
        "outputId": "de839b1d-a1ce-464c-e4a2-161bb1b0262f"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTBC(nn.Module):    # Bert Binary Classification\n",
        "    def __init__(self, bert):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(768, 256)   # bert のモデル Embedding(30522, 768, padding_idx=0) から来ている\n",
        "        self.fc2 = nn.Linear(256, 2)    # Binary Classification\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, cls = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        x = self.fc1(cls)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RhxmBIFEnoJ6"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = BERTBC(bert)"
      ],
      "metadata": {
        "id": "ltpsVG-k3qfX"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "-8eOIIr03sJC"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maJZvBK63xQD",
        "outputId": "42fbd850-b2b2-4d37-f7da-937823dad3ab"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "zBGXr-pV3zJt"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, mask, labels = iter(train_dataloader).next()"
      ],
      "metadata": {
        "id": "iA85MtkB3-nC"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I2lhclb4Pt4",
        "outputId": "2c880c29-be02-4b64-b54d-0737277a729a"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,  9388,  4305, 24665,  3022,  1024,  2081,  1999,  2859,  2003,\n",
              "         2019,  6581,  3185,  2008, 11230,  2129,  2048,  8578,  2031,  2172,\n",
              "         1999,  2691,  2021,  1010,  2024,  2025,  2130,  5204,  1997,  1996,\n",
              "         3747,  2169,  2554,  2038,  2006,  2028,  2178,  1012,  2585,  2417,\n",
              "         8202,  2330,  2115,  2159,  1998,  4473,  2017,  2000,  2156,  2129,\n",
              "         1996,  3667,  1999,  2859, 22027, 17530,  2008,  3465,  2210,  2000,\n",
              "         2498,  1998,  2024,  2853,  1999,  2637,  2005,  2039,  2000,  2322,\n",
              "         6363,  1012,  2043,  2417,  8202,  3980,  4841,  2055,  2073,  2122,\n",
              "        17530,  2272,  2013,  2027,  2018,  2053,  9789,  1998,  2790, 12873,\n",
              "         2631,  1012,  2043,  2002,  2409,  2068,  2008,  2027,  2024,  2081,\n",
              "         1999,  2859,  2005,  2625,  2059,  2498,  2007,  9202,  3477,  1998,\n",
              "        21873,  2551,  3785,  1010,  4841,  2790,  6517,  1010,  3480,  1010,\n",
              "         1998,  1037,  2210, 23124,  3993,  2021,  2134,  1005,  1056,  2428,\n",
              "         4025,  2008,  2027,  2052,  2644, 13131,  1996, 17530,  2044,  4531,\n",
              "         2041,  1996,  3606,  1012,  2043,  2417,  8202,  8781,  1996,  3667,\n",
              "         1999,  2859,  2027,  2106,  2025,  2113,  2008,  4841,  2020,  4147,\n",
              "         2068,  2058,  2037, 26082,  1998,  3825,  2061,  2172,  2005,  2122,\n",
              "        17530,  1012,  1996,  3667,  4191,  2012,  2054,  1996,  3800,  2001,\n",
              "         2369, 17530,  1998,  2481,  1005,  1056,  2903,  2009,  1012,  2023,\n",
              "         3185,  2003,  1037,  2307,  2143,  2008,  3957,  2149,  2242,  2000,\n",
              "         2228,  2055,  1999,  2060,  3032,  4661,  2256,  2219,  1012,  1026,\n",
              "         7987,  1013,  1028,  1026,  7987,  1013,  1028,  1049,  1012, 15091,\n",
              "         2015,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.decode?   # show help\n",
        "tokenizer.decode(input_ids.numpy()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "_ryDcPi044SW",
        "outputId": "ed331857-1500-44a3-bd75-9be9937d27d6"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"[CLS] mardi gras : made in china is an excellent movie that depicts how two cultures have much in common but, are not even aware of the influence each society has on one another. david redmon open your eyes and allows you to see how the workers in china manufactures beads that cost little to nothing and are sold in america for up to 20 dollars. when redmon questions americans about where these beads come from they had no clue and seemed dumb founded. when he told them that they are made in china for less then nothing with horrible pay and unacceptable working conditions, americans seemed sad, hurt, and a little remorseful but didn't really seem that they would stop purchasing the beads after finding out the truth. when redmon questioned the workers in china they did not know that americans were wearing them over their necks and paid so much for these beads. the workers laughed at what the purpose was behind beads and couldn't believe it. this movie is a great film that gives us something to think about in other countries besides our own. < br / > < br / > m. pitts [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNOgWyfV5T8f",
        "outputId": "5d918f51-9c2e-466f-c66c-022a96187e21"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRex6iyc6tJT",
        "outputId": "1933d184-98cd-40de-8713-089a6ecf0c0f"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_neg_dic.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-GvTFar6jUy",
        "outputId": "5f953aea-134a-4204-ea07-cf376a531782"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('neg', 0), ('pos', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Balance the class weight percentage"
      ],
      "metadata": {
        "id": "qPQuMkzq66J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_wts = compute_class_weight(\n",
        "    class_weight = \"balanced\",\n",
        "    classes = np.unique(train['LABEL']),\n",
        "    y = train['LABEL']\n",
        ")\n",
        "# class_wts = dict(zip(np.unique(train['LABEL']), class_wts))\n",
        "\n",
        "weights = torch.tensor(class_wts, dtype=torch.float)\n",
        "# weights = torch.Tensor(class_wts)\n",
        "weights = weights.to(device)\n"
      ],
      "metadata": {
        "id": "o7BMsi3c6nZD"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(class_wts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVNhJ9GQ74dV",
        "outputId": "ebb02311-7449-47dd-d837-55ed0def44be"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define hyper parameters"
      ],
      "metadata": {
        "id": "sIa9YE1t9MHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(weight=weights)"
      ],
      "metadata": {
        "id": "n84Zj9YM9D02"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "learning_rate = 1e-3\n",
        "optimizer = AdamW(net.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "U07ATbHl-beM"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model"
      ],
      "metadata": {
        "id": "3x2OUEc2-qay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def fit():\n",
        "    print('\\nTraining...')\n",
        "    net.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    total_preds = []\n",
        "    t0 = time.time()\n",
        "    elapsed = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 16 == 0 and not step == 0:\n",
        "            elapsed = time.time() - t0\n",
        "            print(f\"Elapsed time:{elapsed:.3f}[sec.]\")\n",
        "            print('Batch {:>5} of {:>5}'.format(step, len(train_dataloader)))\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        input_ids, mask, labels = batch\n",
        "\n",
        "        net.zero_grad()\n",
        "        preds = net(input_ids, mask)\n",
        "        loss = criterion(preds, labels)\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        total_preds.append(preds)\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"Elapsed time:{elapsed:.3f}[sec.]\")\n",
        "    avg_loss = total_loss / len(train_dataloader.dataset)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "mQgzh3OM-m1g"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "    print('\\nEvaluating...')\n",
        "    net.eval()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    total_preds = []\n",
        "    t0 = time.time()\n",
        "    elapsed = 0\n",
        "\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "        if step % 16 == 0 and not step == 0:\n",
        "            elapsed = time.time() - t0\n",
        "            print(f\"Elapsed time:{elapsed:.3f}[sec.]\")\n",
        "            print('Batch {:>5} of {:>5}'.format(step, len(test_dataloader)))\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        input_ids, mask, labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = net(input_ids, mask)\n",
        "            loss = criterion(preds, labels)\n",
        "            total_loss += loss\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            total_preds.append(preds)\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"Elapsed time:{elapsed:.3f}[sec.]\")\n",
        "    avg_loss = total_loss / len(train_dataloader.dataset)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "hN79tta46gKL"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'\\n Epoch:{epoch+1}/{epochs}')\n",
        "    train_loss, _ = fit()\n",
        "    valid_loss, _ = evaluate()\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(net.state_dict(), 'saved_weights.pt')\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(valid_loss)\n",
        "    print(f'\\n Traiing Loss:{train_loss:.3f}, Validation Loss:{valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNhzczg87JLq",
        "outputId": "145adfef-1de5-46f3-a3d8-06867e35a359"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch:1/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.247[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.470[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.040, Validation Loss:0.005\n",
            "\n",
            " Epoch:2/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.224[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.448[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.038, Validation Loss:0.005\n",
            "\n",
            " Epoch:3/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.224[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.447[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.534[sec.]\n",
            "\n",
            " Traiing Loss:0.039, Validation Loss:0.005\n",
            "\n",
            " Epoch:4/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.223[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.445[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.040, Validation Loss:0.005\n",
            "\n",
            " Epoch:5/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.228[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.451[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.037, Validation Loss:0.005\n",
            "\n",
            " Epoch:6/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.224[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.448[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.039, Validation Loss:0.005\n",
            "\n",
            " Epoch:7/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.222[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.444[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.038, Validation Loss:0.005\n",
            "\n",
            " Epoch:8/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.222[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.444[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.036, Validation Loss:0.004\n",
            "\n",
            " Epoch:9/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.224[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.448[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.534[sec.]\n",
            "\n",
            " Traiing Loss:0.036, Validation Loss:0.004\n",
            "\n",
            " Epoch:10/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.229[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.452[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.035, Validation Loss:0.005\n",
            "\n",
            " Epoch:11/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.223[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.445[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.033, Validation Loss:0.004\n",
            "\n",
            " Epoch:12/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.225[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.448[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.035, Validation Loss:0.005\n",
            "\n",
            " Epoch:13/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.223[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.445[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.040, Validation Loss:0.005\n",
            "\n",
            " Epoch:14/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.223[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.445[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.037, Validation Loss:0.004\n",
            "\n",
            " Epoch:15/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.224[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.446[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.034, Validation Loss:0.004\n",
            "\n",
            " Epoch:16/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.223[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.445[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.534[sec.]\n",
            "\n",
            " Traiing Loss:0.033, Validation Loss:0.004\n",
            "\n",
            " Epoch:17/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.226[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.447[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.534[sec.]\n",
            "\n",
            " Traiing Loss:0.032, Validation Loss:0.004\n",
            "\n",
            " Epoch:18/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.223[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.445[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.032, Validation Loss:0.004\n",
            "\n",
            " Epoch:19/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.222[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.444[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.031, Validation Loss:0.004\n",
            "\n",
            " Epoch:20/20\n",
            "\n",
            "Training...\n",
            "Elapsed time:2.223[sec.]\n",
            "Batch    16 of    32\n",
            "Elapsed time:4.445[sec.]\n",
            "\n",
            "Evaluating...\n",
            "Elapsed time:0.533[sec.]\n",
            "\n",
            " Traiing Loss:0.032, Validation Loss:0.004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model"
      ],
      "metadata": {
        "id": "IDMXhI7l9QWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'saved_weights.pt'"
      ],
      "metadata": {
        "id": "KEo682Wf8Rzl"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMluxc99CVjT",
        "outputId": "18cb5c93-dcbf-4f97-be91-5e5c401fdbcb"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    preds = net(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "QWue_DUaDp9l"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = np.array([np.argmax(r) for r in preds])"
      ],
      "metadata": {
        "id": "CDXpYzt8D8kl"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_dXdtZaEGT8",
        "outputId": "38cb97b0-fde6-4cea-fdf9-596c55268742"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.79      0.73        29\n",
            "           1       0.80      0.69      0.74        35\n",
            "\n",
            "    accuracy                           0.73        64\n",
            "   macro avg       0.74      0.74      0.73        64\n",
            "weighted avg       0.74      0.73      0.73        64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "rWzhrsjqES_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "infer_text = [\"I like Disney Movie.\"]\n",
        "ids_mask = tokenizer.batch_encode_plus(infer_text, True, True, True, 256)"
      ],
      "metadata": {
        "id": "UOJIu_ABEPNT"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q1PnHDsEfAZ",
        "outputId": "b9e19fd9-80e0-4c4c-934b-83271149ef5b"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1045, 2066, 6373, 3185, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_mask = ids_mask['input_ids'], ids_mask['attention_mask']"
      ],
      "metadata": {
        "id": "21txGz1tFEDM"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pATrTfQTFJ8w",
        "outputId": "7f65b505-3046-47a4-8cd7-3aca6fa9cf5f"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[101, 1045, 2066, 6373, 3185, 1012, 102]]"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVomxkU2FLGf",
        "outputId": "0af6032d-8e5e-42da-b0d5-4836f5c5c562"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmZBfh6BFLyK",
        "outputId": "e126e66e-4a55-4d3c-daff-886fad24ba88"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = net(torch.tensor(input_ids).to(device), torch.tensor(attention_mask, dtype=torch.long).to(device))"
      ],
      "metadata": {
        "id": "YZ4yTFSTFQPh"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTUTZTObFa1d",
        "outputId": "e063971d-8499-4a75-ba6a-3d1d929887d9"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0166,  0.4855]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IDqK5YEQFboc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}